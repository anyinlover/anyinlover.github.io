"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8130],{77735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/deep_learning/RoPE","metadata":{"permalink":"/blog/deep_learning/RoPE","editUrl":"https://github.com/anyinlover/anyinlover.github.io/blog/blog/deep_learning/RoPE.md","source":"@site/blog/deep_learning/RoPE.md","title":"Rotary Positional Embedding","description":"Introduction to RoPE","date":"2025-04-10T08:56:00.000Z","tags":[{"inline":true,"label":"embedding","permalink":"/blog/tags/embedding"},{"inline":true,"label":"dl","permalink":"/blog/tags/dl"}],"readingTime":4.585,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Rotary Positional Embedding","date":"2025-04-10T08:56:00.000Z","tags":["embedding","dl"]},"unlisted":false,"nextItem":{"title":"Troubleshooting Send to Kindle Conversion Failures","permalink":"/blog/tools/ebook/send_to_kindle_fail"}},"content":"## Introduction to RoPE\\n\\nRope (Rotary Positional Embedding) is a type of relative positional encoding. Currently, most mainstream large models use Rope or its variants. The original paper on Rope can be found in [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864).\\n\\nSince self-attention computation is position-independent, positional encoding has been added since the invention of the transformer to capture dependencies between different positions. The transformer uses absolute positional encoding.\\n\\n$$\\n\\\\begin{aligned}\\np_{k, 2t} &= \\\\sin(k / 10000^{2t / d}) \\\\\\\\\\np_{k, 2t+1} &= \\\\cos(k / 10000^{2t / d})\\n\\\\end{aligned}\\n$$\\n\\nBecause absolute positional encoding is directly added to the token embedding, it cannot directly model the relative positions between tokens. The inference performance on sequences exceeding the training data length drops sharply. Relative positional encoding is used to correct this problem, and Rope has become the mainstream approach.\\n\\nThe core idea of Rope is to find a positional encoding function such that the following equation holds:\\n\\n$$\\n\\\\langle f(\\\\mathbf{q}_m, m), f(\\\\mathbf{k}_n, n) \\\\rangle = g(\\\\mathbf{q}_m, \\\\mathbf{k}_n, m - n)\\n$$\\n\\nThat is, when calculating the dot product of $q$ and $k$ during attention, the result is independent of the absolute positions $m$ and $n$ of the tokens in $q$ and $k$, and only depends on the relative position $m - n$.\\n\\nWhen the embedding dimension $d$ is only 2, the following formulas precisely satisfy the above property:\\n\\n$$\\n\\\\begin{aligned}\\nf(\\\\mathbf{q}_m, m) &= (\\\\mathbf{q}_m)e^{im\\\\theta} \\\\\\\\\\nf(\\\\mathbf{k}_n, n) &= (\\\\mathbf{k}_n)e^{in\\\\theta} \\\\\\\\\\ng(\\\\mathbf{q}_m, \\\\mathbf{k}_n, m - n) &= \\\\text{Re}[(\\\\mathbf{q}_m)(\\\\mathbf{k}_n)^* e^{i(m-n)\\\\theta}]\\n\\\\end{aligned}\\n$$\\n\\nThe proof of the above equation involves the application of complex exponential functions, mainly relying on the following three properties:\\n\\n$$\\n\\\\begin{aligned}\\n\\\\langle z_1, z_2 \\\\rangle &= \\\\text{Re}[z_1 z_2^*] \\\\\\\\\\n(z_1z_2)^* &= z_1^* z_2^* \\\\\\\\\\n(e^{i\\\\phi})^* &= e^{-i\\\\phi}\\n\\\\end{aligned}\\n$$\\n\\nUsing the above three formulas, the Rope formula above can be easily derived.\\n\\nAccording to Euler\'s formula:\\n\\n$$\\ne^{i\\\\phi} = \\\\cos(\\\\phi) + i \\\\sin(\\\\phi)\\n$$\\n\\nExpanding this, we can obtain $f$, which is consistent for both $q$ and $k$:\\n\\n$$\\nf(\\\\boldsymbol{q}_m, m) = \\\\begin{pmatrix} \\\\cos m\\\\theta & -\\\\sin m\\\\theta \\\\\\\\ \\\\sin m\\\\theta & \\\\cos m\\\\theta \\\\end{pmatrix} \\\\begin{pmatrix} q_m^{(1)} \\\\\\\\ q_m^{(2)} \\\\end{pmatrix}\\n$$\\n\\nActually, we can understand the above Rope formula from another more intuitive perspective. The geometric meaning of the dot product of two-dimensional vectors is the product of their lengths multiplied by the cosine of the angle between them. The above Rope positional encoding function is equivalent to rotating the vector while keeping its length unchanged. Therefore, calculating the dot product of two rotated vectors only involves the relative rotation angle and is independent of the absolute angles.\\n\\nOnce the $d=2$ scenario is understood, it becomes relatively easy to understand the scenario where $d$ is any even number. The embedding dimension is divided into pairs, and different $\\\\theta_i = 10000^{-2(i-1)/d}, i \\\\in [1, 2, ..., d/2]$ are applied according to the pair number, resulting in the complete Rope positional encoding function.\\n\\n$$\\n\\\\mathbf{R}_{\\\\boldsymbol{\\\\Theta}, m}^{d} = \\\\begin{pmatrix}\\n\\\\cos m\\\\theta_1 & -\\\\sin m\\\\theta_1 & 0 & 0 & \\\\cdots & 0 & 0 \\\\\\\\\\n\\\\sin m\\\\theta_1 & \\\\cos m\\\\theta_1 & 0 & 0 & \\\\cdots & 0 & 0 \\\\\\\\\\n0 & 0 & \\\\cos m\\\\theta_2 & -\\\\sin m\\\\theta_2 & \\\\cdots & 0 & 0 \\\\\\\\\\n0 & 0 & \\\\sin m\\\\theta_2 & \\\\cos m\\\\theta_2 & \\\\cdots & 0 & 0 \\\\\\\\\\n\\\\vdots & \\\\vdots & \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots & \\\\vdots \\\\\\\\\\n0 & 0 & 0 & 0 & \\\\cdots & \\\\cos m\\\\theta_{d/2} & -\\\\sin m\\\\theta_{d/2} \\\\\\\\\\n0 & 0 & 0 & 0 & \\\\cdots & \\\\sin m\\\\theta_{d/2} & \\\\cos m\\\\theta_{d/2}\\n\\\\end{pmatrix}\\n$$\\n\\nThe core concept of RoPE is to rotate the embedding vectors based on the token\'s position. This is achieved by applying a **rotation matrix** to the token\'s embedding, where the rotation angle is determined by the token\'s position in the sequence. By rotating the embeddings instead of using fixed position encodings, the model can maintain more flexible and continuous position information.\\n\\n## Introduction to YaRN\\n\\nAlthough Rope apply relative position embedding, it is still limit in generalizing past the context windows seen during training. Several extension methods were proposed. YaRN is the most popular one between\\nperformance and complexity. The original paper can be found in [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/abs/2309.00071).\\n\\n\\n$$\\n\\\\theta_i^{new} = \\\\left[ \\\\gamma_i + (1 - \\\\gamma_i) \\\\frac{L}{L\'} \\\\right] \\\\theta_i, \\\\quad \\\\gamma_i = \\\\begin{cases} 1, & r_i > \\\\tau \\\\\\\\ 0, & r_i < 1 \\\\\\\\ \\\\frac{r_i - 1}{\\\\tau - 1}, & \\\\text{others} \\\\end{cases}\\n$$\\n\\nThe key point is that when $\\\\theta_i$ is small enough, it rotate slow, the max rotated angle is $r_i = \\\\frac{\\\\theta_i L}{2\\\\pi}$. if $r_i < 1$, it didn\'t go through the whole cycle during training, so we should interpolate the $\\\\theta_i$. If $r_i > \\\\tau$, it can safely extrapolate. A linear translation is applied between the two conditions.\\n\\nYaRN also add a scale weight to softmax, which is:\\n\\n$$\\n\\\\lambda = \\\\left( 1 + 0.1 \\\\ln \\\\frac{L\'}{L} \\\\right)^2\\n$$\\n\\nIt is just an experience value without theory support.\\n\\n## Open Source Implementation\\n\\n### Transformer-Engine\\n\\n```python\\nclass FusedRoPEFunc(torch.autograd.Function):\\n    def forward(\\n        ctx,\\n        t: torch.Tensor,\\n        freqs: torch.Tensor,\\n        tensor_format: str = \\"sbhd\\",\\n        interleaved: bool = False,\\n        cu_seqlens: Union[torch.Tensor, None] = None,\\n        cp_size: int = 1,\\n        cp_rank: int = 0,\\n    ) -> torch.Tensor:\\n    def backward(ctx, grad_output: torch.Tensor) -> Tuple[Union[torch.Tensor, None], ...]:\\n```\\n\\n### Flash-Attention\\n\\n```python\\nclass ApplyRotaryEmb(torch.autograd.Function):\\n    def forward(\\n        ctx,\\n        x,\\n        cos,\\n        sin,\\n        interleaved=False,\\n        inplace=False,\\n        seqlen_offsets: Union[int, torch.Tensor] = 0,\\n        cu_seqlens: Optional[torch.Tensor] = None,\\n        max_seqlen: Optional[int] = None,\\n    ):\\n\\n    def backward(ctx, do):\\n```"},{"id":"/tools/ebook/send_to_kindle_fail","metadata":{"permalink":"/blog/tools/ebook/send_to_kindle_fail","editUrl":"https://github.com/anyinlover/anyinlover.github.io/blog/blog/tools/ebook/send_to_kindle_fail.md","source":"@site/blog/tools/ebook/send_to_kindle_fail.md","title":"Troubleshooting Send to Kindle Conversion Failures","description":"Occasionally, users encounter difficulties when attempting to send EPUB files to their Kindle devices using Amazon\'s \\"Send to Kindle\\" service. The underlying causes for these conversion failures remain elusive, presenting a challenge for consistent troubleshooting.","date":"2025-03-18T08:05:00.000Z","tags":[{"inline":true,"label":"Epub","permalink":"/blog/tags/epub"},{"inline":true,"label":"Kindle","permalink":"/blog/tags/kindle"}],"readingTime":0.71,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Troubleshooting Send to Kindle Conversion Failures","date":"2025-03-18T08:05:00.000Z","tags":["Epub","Kindle"]},"unlisted":false,"prevItem":{"title":"Rotary Positional Embedding","permalink":"/blog/deep_learning/RoPE"},"nextItem":{"title":"Floating-Point Representation A Deep Dive","permalink":"/blog/cpp/floating_point"}},"content":"Occasionally, users encounter difficulties when attempting to send EPUB files to their Kindle devices using Amazon\'s \\"Send to Kindle\\" service. The underlying causes for these conversion failures remain elusive, presenting a challenge for consistent troubleshooting.\\n\\nHowever, a practical workaround often proves effective:\\n\\n**Intermediate AZW3 Conversion:**\\n\\n* Convert the problematic EPUB file to the AZW3 format, which is a Kindle-native format.\\n* Subsequently, convert the AZW3 file back to EPUB.\\n\\nThis two-step conversion process can effectively address certain compatibility issues that may be hindering the initial \\"Send to Kindle\\" conversion. By introducing this intermediate format, the file undergoes a re-processing that often resolves underlying formatting or encoding conflicts.\\n\\nWhile this workaround is frequently successful, it\'s important to note that the specific reasons for the original conversion failures can vary. Amazon\'s conversion algorithms and the inherent complexities of EPUB formatting contribute to this variability."},{"id":"/cpp/floating_point","metadata":{"permalink":"/blog/cpp/floating_point","editUrl":"https://github.com/anyinlover/anyinlover.github.io/blog/blog/cpp/floating_point.md","source":"@site/blog/cpp/floating_point.md","title":"Floating-Point Representation A Deep Dive","description":"IEEE 754 Floating-Point Standard","date":"2025-03-14T14:43:00.000Z","tags":[{"inline":true,"label":"float","permalink":"/blog/tags/float"},{"inline":true,"label":"precision","permalink":"/blog/tags/precision"}],"readingTime":4.305,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Floating-Point Representation A Deep Dive","date":"2025-03-14T14:43:00.000Z","tags":["float","precision"]},"unlisted":false,"prevItem":{"title":"Troubleshooting Send to Kindle Conversion Failures","permalink":"/blog/tools/ebook/send_to_kindle_fail"},"nextItem":{"title":"OpenCore Linux DualBoot Setting","permalink":"/blog/tools/os/opencore_linux_dualboot"}},"content":"## IEEE 754 Floating-Point Standard\\n\\nThe IEEE 754 standard defines how floating-point numbers are represented in computers. A number, $V$, is expressed as:\\n\\n$$\\nV = (-1)^s \\\\times M \\\\times 2^E\\n$$\\n\\nWhere:\\n\\n* **$s$ (Sign):** Determines the sign of the number: $s = 0$ for positive, $s = 1$ for negative.\\n* **$M$ (Significand/Mantissa):** A fractional binary number. It ranges from $1$ to $2 - \\\\epsilon$ for normalized values, or from $0$ to $1 - \\\\epsilon$ for denormalized values, where $\\\\epsilon$ is the machine epsilon.\\n* **$E$ (Exponent):** Weights the value by a power of 2.\\n\\n### Common Floating-Point Formats\\n\\nThe following table summarizes the key characteristics of common floating-point formats:\\n\\n| Format | Total Bits | Exponent Bits ($k$) | Fraction Bits ($n$) |\\n| :----- | :--------- | :------------------ | :------------------ |\\n| Double | 64         | 11                  | 52                  |\\n| Float  | 32         | 8                   | 23                  |\\n| FP16   | 16         | 5                   | 10                  |\\n| BF16   | 16         | 8                   | 7                   |\\n\\n### Special Value Categories\\n\\nFloating-point numbers can represent various special values, defined by the exponent ($e$) and fraction ($f$) fields:\\n\\n| Category            | Condition                 | Value                                       |\\n| :------------------ | :------------------------ | :------------------------------------------ |\\n| Normalized Values   | $0 < e < 2^k - 1$         | $(-1)^s \\\\times (1 + f) \\\\times 2^{e - bias}$ |\\n| Denormalized Values | $e = 0$                   | $(-1)^s \\\\times f \\\\times 2^{1 - bias}$       |\\n| Infinity            | $e = 2^k - 1$, $f = 0$    | $(-1)^s \\\\times \\\\infty$                      |\\n| NaN (Not a Number)  | $e = 2^k - 1$, $f \\\\neq 0$ | NaN                                         |\\n\\nWhere the bias is $2^{k-1} - 1$.\\n\\n**Denormalized numbers** serve two crucial purposes:\\n\\n1.  **Representation of Zero:** They allow for distinct representations of positive ($+0.0$) and negative ($-0.0$) zero, differentiated by the sign bit.\\n2.  **Representation of Values Close to Zero:** They enable the representation of numbers very close to $0.0$, filling the gap between zero and the smallest normalized number.\\n\\n### Example: 6-Bit Floating-Point Format\\n\\nLet\'s illustrate with a 6-bit floating-point format (1 sign bit, 3 exponent bits, 2 fraction bits):\\n\\n| Description          | Bit Representation | $E$  | $f$  | Value    |\\n| :------------------- | :----------------- | :--- | :--- | :------- |\\n| Zero                 | 0 000 00           | -3   | 0/4  | 0/32     |\\n| Smallest Positive    | 0 000 01           | -3   | 1/4  | 1/32     |\\n| Largest Denormalized | 0 000 11           | -3   | 3/4  | 3/32     |\\n| Smallest Normalized  | 0 001 00           | -2   | 0/4  | 4/32     |\\n| One                  | 0 011 00           | 0    | 0/4  | 4/4      |\\n| Largest Normalized   | 0 110 11           | 3    | 3/4  | 28/4     |\\n| Infinity             | 0 111 00           | -    | -    | $\\\\infty$ |\\n\\n## Rounding Modes\\n\\nWhen a number cannot be represented exactly, rounding is necessary. Common rounding modes include:\\n\\n| Mode               | 1.4  | 1.6  | 1.5  | 2.5  | -1.5 |\\n| :----------------- | :--- | :--- | :--- | :--- | :--- |\\n| Round-to-Even      | 1    | 2    | 2    | 2    | -2   |\\n| Round-Toward-Zero  | 1    | 1    | 1    | 2    | -1   |\\n| Round-Down (Floor) | 1    | 1    | 1    | 2    | -2   |\\n| Round-Up (Ceiling) | 2    | 2    | 2    | 3    | -1   |\\n\\n## Floating-Point Operations: Precision and Pitfalls\\n\\nA significant challenge with floating-point arithmetic is the \\"big eats small\\" phenomenon. Consider:\\n\\n$$\\n3.14 + 1 \\\\times 10^{10} - 1 \\\\times 10^{10} = 0.0\\n$$\\n\\nThis occurs due to the following steps in floating-point addition:\\n\\n1.  **Alignment:** Exponents are aligned by shifting the significand of the smaller number until both exponents match.\\n2.  **Significand Addition:** The significands are added.\\n3.  **Normalization and Rounding:** The result is normalized and rounded if necessary.\\n\\nPrecision loss happens during the alignment step when one number is significantly larger than the other.\\n\\n### Python Representation of Floating-Point Numbers\\n\\nThe following Python code demonstrates how to decompose a float into its significand and exponent:\\n\\n```python\\nimport struct\\n\\ndef float_to_fe(f):\\n    packed = struct.pack(\'>f\', f)\\n    int_val = struct.unpack(\'>I\', packed)[0]\\n    sign = (int_val >> 31) & 1\\n    exponent = (int_val >> 23) & 0xFF\\n    mantissa = int_val & 0x7FFFFF\\n\\n    if exponent == 0xFF:  # Infinity or NaN\\n        if mantissa == 0:\\n            return \\"Infinity\\" if sign == 0 else \\"-Infinity\\"\\n        else:\\n            return \\"NaN\\"\\n\\n    bias = 127\\n    if exponent == 0:\\n        e = 1 - bias\\n        mantissa_binary = f\\"0.{mantissa:023b}\\" #denormalized\\n    else:\\n        e = exponent - bias\\n        mantissa_binary = f\\"1.{mantissa:023b}\\" #normalized\\n\\n    if sign == 1:\\n        mantissa_binary = \\"-\\" + mantissa_binary\\n\\n    return f\\"{mantissa_binary} * 2^{e}\\"\\n```\\n\\nExample:\\n\\n$$\\n3.14 = 1.10010001111010111000011 \\\\times 2^1 \\\\\\\\\\n1 \\\\times 10^{10} = 1.00101010000001011111001 \\\\times 2^{33}\\n$$\\n\\nTo align $3.14$ with $1 \\\\times 10^{10}$, its significand must be right-shifted by 32 bits. Due to the 23-bit fraction field in single-precision floats, $3.14$ effectively becomes $0.0$.\\n\\n## Conversion Between Floating-Point Formats\\n\\nConverting between floating-point formats can lead to:\\n\\n* **Overflow:** If the target format\'s exponent range is smaller.\\n* **Loss of Precision/Underflow:** If the target format\'s fraction field is smaller."},{"id":"/tools/os/opencore_linux_dualboot","metadata":{"permalink":"/blog/tools/os/opencore_linux_dualboot","editUrl":"https://github.com/anyinlover/anyinlover.github.io/blog/blog/tools/os/opencore_linux_dualboot.md","source":"@site/blog/tools/os/opencore_linux_dualboot.md","title":"OpenCore Linux DualBoot Setting","description":"As Apple and Homebrew ceased support for my 2015 MacBook Pro, I turned to OpenCore Patcher to ensure continued functionality. However, recently, many Python packages (pypi) no longer support Intel macOS versions, leading me to free approximately 400GB of disk space and instal Linux Mint alongside macOS.","date":"2025-02-27T14:43:00.000Z","tags":[{"inline":true,"label":"OpenCore","permalink":"/blog/tags/open-core"},{"inline":true,"label":"Linux","permalink":"/blog/tags/linux"},{"inline":true,"label":"BootLoader","permalink":"/blog/tags/boot-loader"}],"readingTime":1.32,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"OpenCore Linux DualBoot Setting","date":"2025-02-27T14:43:00.000Z","tags":["OpenCore","Linux","BootLoader"]},"unlisted":false,"prevItem":{"title":"Floating-Point Representation A Deep Dive","permalink":"/blog/cpp/floating_point"}},"content":"As Apple and Homebrew ceased support for my 2015 MacBook Pro, I turned to OpenCore Patcher to ensure continued functionality. However, recently, many Python packages (pypi) no longer support Intel macOS versions, leading me to free approximately 400GB of disk space and instal Linux Mint alongside macOS.\\n\\nUpon installation, the OpenCore Patcher boot menu unexpectedly disappeared after rebooting into Linux Mint. This issue arises because Linux Mint put it the first one in the boot order. Each operating system\u2014whether it\u2019s macOS or Linux\u2014installs its own bootloader in the EFI partition. In my case, the EFI partition was mounted under `/boot/efi` when in Linux Mint.\\n\\nTo resolve this, I followed the easiest way in [OpenCore MultiBoot](https://dortania.github.io/OpenCore-Multiboot/oc/linux.html)\\n\\n1. **Backup and Modify the OpenCore Config File:**\\n   - The original configuration file for OpenCore is located at `/boot/efi/EFI/OC/config.plist`.\\n   - Backup this file before making any changes.\\n   - Navigate to `Misc -> BlessOverride` within the config.plist editor.\\n   - Change the value from `\\\\EFI\\\\Microsoft\\\\Boot\\\\bootmgfw.efi` to `\\\\EFI\\\\ubuntu\\\\grubx64.efi`.\\n\\n2. **Restart and Access OpenCore Menu:**\\n   - After making these changes, restart your computer while holding down the Option key.\\n   - From the boot menu, select \\"OpenCore\\" to access the EFI menu.\\n\\n3. **Set OpenCore as Default Bootloader:**\\n   - Once inside the OpenCore menu, it will automatically set as the first entry in the boot order.\\n   - This means you won\u2019t need to press Option every time you reboot your system.\\n\\nBy following these steps, I have successfully restored the functionality of my dual-boot setup, ensuring a seamless transition between macOS and Linux Mint while retaining control over which operating system boots by default."}]}}')}}]);