# Megatron

## Systematic skimming

1. What problem does this project want to solve?

How to train large transformer language models at scale more efficiently?

2. What creative points do this project have?

It implements pipeline parallel, tensor parallel, activation recompution. It also implements a new data parallel besides the torch one. It integrate Zero tech and FlashAttention.

3. What is the structure of this project?

The core codes are in /core, it implements the major model training like GPT, Bert, Llama, and the recent Retro.

The total lines num is about 40k, the lines num in core is about 10k.

## Superficial reading

1.Which components are the important ones to solve the problems?

- pipeline parallel -- core/pipeline_parallel
- [[Tensor Parallel]] -- core/tensor_parallel
- data parallel -- core/distributed
- activation recompution -- core/transformer
- zero distributed optimizer -- optimizer

2.what coding style and design patterns does the project use?

factory method, metaprogramming(hasattr, setattr), common

3.What users' contracts does the project have?

- data preprocessing -- tools/preprocess_data.py
- distributed training -- examples/pretrain_{bert,gpt,t5}_distributed.sh
- evaluation -- tasks/main.py


[//begin]: # "Autogenerated link references for markdown compatibility"
[Tensor Parallel]: <Tensor Parallel.md> "Tensor Parallel"
[//end]: # "Autogenerated link references"