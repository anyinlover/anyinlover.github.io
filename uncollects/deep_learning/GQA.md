# GQA

GQA, on the other hand, is a specific type of attention mechanism proposed as an alternative to MHA([[Multi-Head Attention]]). GQA tries to reduce the computational burden while still being able to capture global interactions by using a single attention mechanism.

![gqa](../../assets/image/gqa.png)


1. Ainslie, Joshua, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. “GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints,” May 22, 2023. https://arxiv.org/abs/2305.13245v3.
