# 注意力机制

注意力机制是受认知神经科学的启发构造的模型，核心思想就是邻近数据影响力最大。

## 注意力提示

在生活中，我们可能不自觉的被电视所吸引，也可能主动的去看书，这就是非自主性提示和自主性提示之分。电视是非自主性提示，而书是自主性提示。

对于全连接层而言，其中只有非自主性提示。而注意力机制中还包含了自主性提示。

注意力机制通过注意力汇聚将查询（自主性提示）和键（非自主性提示）结合在一起，实现对值（感官输入）的选择倾向。

![注意力机制](https://zh-v2.d2l.ai/_images/qkv.svg)

## 注意力汇聚

注意力机制是深度学习的新模型，但类似的思想其实很早之前就有了。Nadaraya-Watson核回归的思路已经和注意力机制比较接近了。

最简单的注意力汇聚是不考虑$x$的分布，直接对输出值进行平均汇聚，但这种效果肯定不好。

$$ f(x) = \frac{1}{n} \sum_{i=1}^n y_i $$

Nadaraya-Watson核回归在此思路上更近一步，利用核函数和softmax，根据输入位置对输出进行加权，可以比较好的拟合。

$$ f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i $$

对此进行扩展，就得到了一个通用的注意力汇聚公式，其中$\alpha (x, x_i)$为注意力权重，满足概率分布。

$$ f(x) = \sum_{i=1}^n \alpha (x, x_i)y_i $$

当核函数取高斯核时，$K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2})$，可以得到：

$$
\begin{align}
f(x) &= \sum_{i=1}^n \alpha (x, x_i)y_i \\
&= \sum_{i=1}^n \frac{\exp(-\frac{1}{2}(x - x_i)^2)}{\sum_{j=1}^n \exp(-\frac{1}{2}(x - x_j)^2)} y_i \\
&= \sum_{i=1}^n \rm{softmax}(-\frac{1}{2}(x - x_i)^2)y_i
\end{align}
$$

从上式可以直观的看到，键$x_i$越接近查询$x$，对应的值$y_i$分配到的注意力权重越大。

上面的模型时非参数化的，也可以轻松的扩展到参数化模型。

$$
\begin{align}
f(x) &= \sum_{i=1}^n \alpha (x, x_i)y_i \\
&= \sum_{i=1}^n \frac{\exp(-\frac{1}{2}((x - x_i)w)^2)}{\sum_{j=1}^n \exp(-\frac{1}{2}((x - x_j)w)^2)} y_i \\
&= \sum_{i=1}^n \rm{softmax}(-\frac{1}{2}((x - x_i)w)^2)y_i
\end{align}
$$

## 注意力评分函数

注意力所做事可以用下面的图来表示：

![注意力汇聚加权和](https://zh.d2l.ai/_images/attention-output.svg)

对于查询 $\mathbf{q} \in \mathbb{R}^q$ 和 $m$ 个“键-值”对 $(\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)$，其中 $\mathbf{k}_i \in \mathbb{R}^k$，$\mathbf{v}_i \in \mathbb{R}^v$，注意力汇聚函数定义为：

$$f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i)$$

其中注意力权重由注意力评分函数$a(\mathbf{q}, \mathbf{k}_i)$经过softmax运算得到：

$$
\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_{j=1}^m \exp(a(\mathbf{q}, \mathbf{k}_j))}
$$

### 掩蔽softmax操作

除了标准的softmax操作，还有一类掩蔽softmax操作，通过指定有效序列长度来过滤掉超出指定范围的数据。

不同的注意力评分函数会导致不同的注意力汇聚操作。下面是两个流行的评分函数。

### 加性注意力

当查询和键不同长度时，适合使用加性注意力：

$$a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k)$$

其中可学习的参数是 $\mathbf W_q\in\mathbb R^{h\times q}$、$\mathbf W_k\in\mathbb R^{h\times k}$ 和 $\mathbf w_v\in\mathbb R^{h}$。将查询和键连接起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数 $h$。通过使用 $\tanh$ 作为激活函数，并且禁用偏置项。

### 缩放点积注意力

当查询和键具有相同长度$d$时，假设查询和键的所有元素都是满足零均值和单位方差的独立随机变量。则两个向量的点积的均值为 $0$，方差为 $d$。为确保无论向量长度如何，点积的方差在不考虑向量长度的情况下仍然是 $1$，有下面的缩放点积注意力：

$$a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}$$

## Bahdanau注意力

Bahdanau注意力其实就是把注意力机制引入了经典的seq2seq编解码架构中。根据直觉，在预测词元的时候，只有一小部分对应的输入序列与此相关。在原来的架构中，上下文变量$\mathbf{c}$ 在任何解码时间都保持不变，而在Bahdanau注意力中它在步骤 $t'$ 都会被 $\mathbf{c}_{t'}$ 替换：

$$\mathbf{c}_{t'} = \sum_{t=1}^T \alpha(\mathbf{s}_{t' - 1}, \mathbf{h}_t) \mathbf{h}_t$$

![Bahdanau注意力结构](https://zh-v2.d2l.ai/_images/seq2seq-attention-details.svg)

## 多头注意力

多头注意力的想法其实也简单，就是用不同的注意力学习不同的行为，最后做一个拼接。相同的查询、键、值通过学习得到不同的线性投影分别送入不同的注意力中。最后不同的注意力再做一个连结。

![多头注意力](https://zh-v2.d2l.ai/_images/multi-head-attention.svg)

给定查询 $\mathbf{q} \in \mathbb{R}^{d_q}$、键 $\mathbf{k} \in \mathbb{R}^{d_k}$ 和值 $\mathbf{v} \in \mathbb{R}^{d_v}$，每个注意力头 $\mathbf{h}_i$ ($i = 1, \ldots, h$) 的计算方法为：

$$\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v},$$

其中，可学习的参数包括 $\mathbf W_i^{(q)}\in\mathbb R^{p_q\times d_q}$、$\mathbf W_i^{(k)}\in\mathbb R^{p_k\times d_k}$ 和 $\mathbf W_i^{(v)}\in\mathbb R^{p_v\times d_v}$ ，以及代表注意力汇聚的函数 $f$ 。$f$ 可以是加性注意力或缩放点积注意力。多头注意力的输出需要经过另一个线性转换，它对应着 $h$ 个头连结后的结果，因此其可学习参数是 $\mathbf W_o\in\mathbb R^{p_o\times h p_v}$：

$$\mathbf W_o \begin{bmatrix}\mathbf h_1\\\vdots\\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}.$$

## 自注意力和位置编码

当输出序列长度和输入序列相同时，所谓的自注意力即使用同一组词元序列作为查询、键和值。

给定一个由词元组成的输入序列 $\mathbf{x}_1, \ldots, \mathbf{x}_n$，其中任意 $\mathbf{x}_i \in \mathbb{R}^d$ ($1 \leq i \leq n$)。该序列的自注意力输出为一个长度相同的序列 $\mathbf{y}_1, \ldots, \mathbf{y}_n$，其中：

$$\mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \ldots, (\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d$$

自注意力有很好的并行度，但同时也丢掉了位置信息。一种解决方式是在输入中加入位置编码矩阵。

假设输入表示 $\mathbf{X} \in \mathbb{R}^{n \times d}$ 包含一个序列中 $n$ 个词元的 $d$ 维嵌入表示。位置编码使用相同形状的位置嵌入矩阵 $\mathbf{P} \in \mathbb{R}^{n \times d}$ 输出 $\mathbf{X} + \mathbf{P}$，矩阵第 $i$ 行、第$2j$列和$2j$ 列上的元素为：

$$\begin{aligned} p_{i, 2j} &= \sin\left(\frac{i}{10000^{2j/d}}\right),\\p_{i, 2j+1} &= \cos\left(\frac{i}{10000^{2j/d}}\right).\end{aligned}$$

这种设计能保证每个样本不同位置的位置编码是不同的，同时还有一个比较有趣的相对位置编码性质，对于任何确定的位置偏移 $\delta$，位置 $i + \delta$ 处的位置编码可以线性投影位置 $i$ 处的位置编码来表示。

令 $\omega_j = 1/10000^{2j/d}$，对于任何确定的位置偏移 $\delta$，任何一对 $(p_{i, 2j}, p_{i, 2j+1})$ 都可以线性投影到 $(p_{i+\delta, 2j}, p_{i+\delta, 2j+1})$：

$$\begin{aligned}
&\begin{bmatrix} \cos(\delta \omega_j) & \sin(\delta \omega_j) \\  -\sin(\delta \omega_j) & \cos(\delta \omega_j) \\ \end{bmatrix}
\begin{bmatrix} p_{i, 2j} \\  p_{i, 2j+1} \\ \end{bmatrix}\\
=&\begin{bmatrix} \cos(\delta \omega_j) \sin(i \omega_j) + \sin(\delta \omega_j) \cos(i \omega_j) \\  -\sin(\delta \omega_j) \sin(i \omega_j) + \cos(\delta \omega_j) \cos(i \omega_j) \\ \end{bmatrix}\\
=&\begin{bmatrix} \sin\left((i+\delta) \omega_j\right) \\  \cos\left((i+\delta) \omega_j\right) \\ \end{bmatrix}\\
=&
\begin{bmatrix} p_{i+\delta, 2j} \\  p_{i+\delta, 2j+1} \\ \end{bmatrix},
\end{aligned}$$

## Transformer

Transformer是在编解码架构上跳过了RNN，直接使用注意力机制。由于自注意力同时具有并行计算和最短的最大路径长度这两个优势，Transformer的效果还是非常不错的。

其模型结构如下图所示：

![Transformer网络图](https://zh-v2.d2l.ai/_images/transformer.svg)

其编码器和解码器各有n个相同的层叠加而成。

在编码器侧，

- 多头注意力是之前已经讲过的
- 逐位前馈网络是一个MLP用于位置变换
- 加法和规范化中，加法其实就是一个残差连接，规范化是一类层规范化

编码器侧：

- 掩蔽多头解码器注意力，只保留已生成词元位置。
- 逐位前馈网络与编码器一致
- 编码器-解码器多头注意力，查询来自前一个解码器层输出，键和值来自整个编码器输出。
- 加法和规范化和编码器一致。
