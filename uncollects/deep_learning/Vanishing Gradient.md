# Vanishing Gradient

In deep networks, the gradients in optimization algorithm like [[Stochastic Gradient Descent]] can become very small as they are backpropagated through many layers. This is known as the vanishing gradient problem.
Small gradients make it difficult for the earlier layers in the network to learn effectively. They receive little information about how to adjust their parameters to reduce the loss.

1. [Gemini](https://gemini.google.com)
