# GPT

GPT is a [[Language Model]] that just use the decoder of a [[Transformer]]. Below is its neural network architecture.

![gpt](../../assets/image/gpt.jpeg)

It takes an input sequence and predicts the next word in the sequence, and then the next, and so on.

```python
class GPTBlock(nn.Module):
    def __init__(self, emb_size, n_head, dropout, bias):
        super().__init__()
        self.attn = nn.MultiheadAttention(embed_dim=emb_size, num_heads=n_head, batch_first=True)
        self.ffn = nn.Sequential(
            nn.Linear(emb_size, 4 * emb_size),
            nn.GeLU(),
            nn.Linear(4 * emb_size, emb_size),
        )
        self.ln1 = nn.LayerNorm(emb_size, bias=bias)
        self.ln2 = nn.LayerNorm(emb_size, bias=bias)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x_attn = self.ln1(x)
        attn_out, _ = self.attn(x_attn, x_attn, x_attn)
        x = x + self.dropout(attn_out)
        x_ffn = self.ln2(x)
        ff_out = self.ffn(x_ffn)
        x = x + self.dropout(ff_out)
        return x


class GPT(nn.Module):
    def __init__(self, vocab_size, emb_size, n_layer, n_head, dropout, bias, max_position_embeddings=512):
        super().__init__()
        self.token_embeddings = nn.Embedding(vocab_size, emb_size)
        self.position_embeddings = nn.Embedding(max_position_embeddings, emb_size)
        self.embedding_dropout = nn.Dropout(dropout)
        self.layers = nn.ModuleList([GPTBlock(emb_size, n_head, dropout, bias) for _ in range(n_layer)])
        self.lm_head = nn.Linear(emb_size, vocab_size, bias=False)
        self.token_embeddings.weight = self.lm_head.weight

    def forward(self, x, targets=None):
        batch_size, seq_len = x.shape
        positions = torch.arange(0, seq_len, dtype=torch.long, device=x.device)
        token_embeds = self.token_embeddings(x)
        position_embeds = self.position_embeddings(positions)
        x = self.embedding_dropout(token_embeds + position_embeds)

        for layer in self.layers:
            x = layer(x)

        if targets is not None:
            logits = self.lm_head(x)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
            return logits, loss
        else:
            logits = self.lm_head(x[:, -1, :])
            loss = None
            return logits, loss
```

[//begin]: # "Autogenerated link references for markdown compatibility"
[Language Model]: <Language Model.md> "Language Model"
[Transformer]: ../deep_learning/Transformer.md "Transformer"
[//end]: # "Autogenerated link references"
