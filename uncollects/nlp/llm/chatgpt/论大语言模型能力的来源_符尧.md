# 大语言模型能力的来源

2023.3.15 符尧

可同步参考[博客](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)

GPT4

predict the model before training it. 训练前就已经预测到会变强
scaling

* coding
* math and natural science
* domain knowledge
* reasoning
* better generalization

longer context 4000 -> 32000
vision-language reasoning
efficiency

Scaling law
log-linear

pretraining tokens
fine-tuning tokens
input context window
type of Instruction
outside memory

in-context perf
zero-shot perf
fine-tuning perf
in-dist perf
OOD perf

Emergent Abilities
phase change

PaLM -> FlanPaLM -> 
Gopher/Chinchilla -> Sparrow
GPT3 -> InstructGPT -> ChatGPT

## Pretraining

Leaderboard - MMLU
GPT4
GPT3.5
PaLM
Chinchilla
LLaMA
Gopher
Galactica
GLM-130B
BLOOM
OPT

Abilities
Language generation
World knowledge
In-context learning
Code Understanding/generation
Complex reasoning/ chain-of-thought

## Instruction Tuning
Goal: unlock model Abilities

Leaderboard - MMLU
Text-davinci-002/003
Flan-PaLM
OPT-IML
LM self-instruct
MOSS

Abilities
Follow instructions
Zero-shot generation, no in-context
Generalize to unseen instructions/ tasks
Compositional generalization
Complex reasoning / Chain-of-thought

## Alignment
Goal: align with human value system

Models:
OpenAI - ChatGPT
DeepMind - Sparrow
Anthropic - Claude

Abilities

Informative and useful responses
Inpartial responses
Reject improper queries
Reject unknown knowledge

supervised/RLHF

## Specialization

是否可能平衡模型多个能力，让模型专业化。

去年10月openAI reward模型规模增大对效果的影响

11B Flan-T5
16B Nama

