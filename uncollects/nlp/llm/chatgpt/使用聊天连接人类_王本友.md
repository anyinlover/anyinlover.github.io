# 使用聊天连接人类

2023.3.16 王本友

为什么可扩展

网络可拓展 Transformer vs CNN/RNN
目标可拓展 conditional regressive LM vs Masked LM
数据可拓展 plain texts vs supervised data

为什么要大模型

1. 世界知识
2. 问题解决能力
3. 更好的泛化
4. 涌现能力

Prompt Engineering

In-context Learning
Chaim of thoughts
Interactive prompt
learn to generate/retrieve prompt

成本 推理一次0.01美元
推理 单机8卡 10分钟
数据清洗，过滤，选择策略
人才

能不能跟上ChatGPT

时间 matters
80%问题不大，完全追上很难
单轮基本还行，多轮有点难度

未来追赶上的机会
AGI遇到瓶颈

捷径

用一个初始化好的开源GPT3模型
ChatGPT反馈替代人类反馈

国内没有高质量数据，产生恶性循环

百度文心一言 多轮弱，单轮70-80%

大模型未解之谜
Reasoning来源？
Emergent ability？
Where is its border
Alignment makes it generalize better?
why so long context
Stil scale up?
Agi?

什么可以做
Efficiency
Specialization/Personalization


参数压缩
知识注入
