# ChatGPT测试与评估

2013.3.13 冯熙栋

## 语言模型的测试与评估标准

评估方式
1. 人工评估
2. 自动评估

评估指标
1. 评估分数
2. 有害性
3. 模型效率
4. 模型鲁棒性
5. 多样性

常用评估分数

1. Accuracy Precision Recall F1
2. BLEU 语言模型生成的文本与参考文本的相似度 机器翻译任务常用评估指标
3. ROUGE 语言模型生成的文本与参考文本的相似度 文本摘要任务常用评估指标
4. Distinct 语言模型生成文本多样性指标 衡量不重复ngram数量
5. Perplexity 语言模型的困惑度 对一个给定测试集的预测能力指标。越低越好 常用于生成任务

## 评估综述

翻译：

1. 对提示有一定的敏感性
2. 多语言翻译，对语料较为丰富的场景，与商用软件科比。对语料缺少的环境，差距比较明显。
3. 鲁棒性不佳，对于文本中存在错误或者需要领域知识的情况低于商业软件。

通用能力：
1. 20 NLP任务
2. 涉及算术推理常识推理符号推理
3. 逻辑推理问答总结情感分析

zero-shot强，但没有超过fine-tune模型
chatgpt和gpt-3.5擅长不同， 前者擅长对话，语言理解和问答，后者擅长推理
思维链可显著提升推理能力
具有初步绘画能力，比较初级
在低资源语言中效果降幅明显
语义问题可以很好的解决
语用问题随问题难度上升而下降
一般会给出相对保守负面的评估

推理能力
1. 不擅长数学推理与空间推理
2. 擅长溯因和演绎推理，不擅长归纳推理
3. 有一定时序推理常识推理因果推理类比推理能力

认知幻觉
存在对事实数据的认知错误
外部认知幻觉更强

交互性
交互能力极强
多轮prompt修正，对返回结果进行反馈，结果增益极高

鲁棒性
few-shot中1-shot表现最好
受对抗供给后掉点严重
对提示/数字及任务标签表现敏感，
在绝大部分任务上鲁棒程度超过其他开源模型，但其绝对性能离完美差异很大

事件抽取
在事件抽取任务上智能达到该领域专门模型效果的一半

负面情绪检测
ChatGPT比人工标注人员表现更稳定，不容易受上下文影响

专业能力
数学不行，方差极大

医学领域
临床病例的生成，修正与评估，在适当的审查和注意事项下，具备此项能力
ChatGPT未通过医学生能力，美国勉强通过医学资格考试
ChatGPT可以生成如此以假乱真的医学领域论文摘要

法律领域
法学院考试平均表现c+，达到及格水平
具有一定的法律写作能力


不足：

1. 推理能力不足
2. 事实错误
3. 偏见与歧视
4. 透明性与可信度
5. 道德水平表现高度不一致（电车难题）


