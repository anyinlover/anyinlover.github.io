# 有限马尔可夫决策过程

## 马尔可夫决策过程的定义

马尔可夫决策过程是在数学上对序列决策的抽象化。其中有限马尔可夫决策过程可以很好的刻画强化学习的数学性质，这里的有限指的是状态，动作和奖励均是有限的集合。我们说一个系统具有马尔可夫性质，是说这个系统下一时刻的状态只受到当前状态的影响，所有历史状态的信息都隐含在当前状态下。

在强化学习中，我们定义了智能体与环境的区分，智能体是决策主体，其他一切都属于环境。智能体从环境中接受到状态，并作出决策动作。因此接收到奖励和下一个状态。注意有时候需要区分状态和观察值。智能体可能无法感知到环境所有的状态值，而只能部分感知。

在有限马尔可夫决策过程中，状态和奖励都根据上一刻的状态和动作服从固定的概率分布：

$$p(s',r | s,a) \doteq \Pr\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\}$$

其中$s',s \in \mathcal{S}$,$r \in \mathcal{R}$, $a \in \mathcal{A}(s)$。这里的$p$刻画了这个马尔可夫决策过程的动力学。

从上面的四元动力学函数，可以推导出其他刻画环境的变量。

比如下面是状态转移概率分布：

$$p(s'|s,a) \doteq \Pr\{S_t = s' | S_{t-1} = s, A_{t - 1}=a\} = \sum_{r\in\mathcal{R}}p(s',r|s,a)$$

期望奖励：

$$r(s,a) \doteq \mathbb{E}[R_t | S_{t-1}=s, A_{t-1}=a] = \sum_{r\in\mathcal{R}}r \sum_{s'\in\mathcal{S}} p(s',r|s,a)$$

确定状态的期望奖励：

$$r(s,a, s') \doteq \mathbb{E}[R_t | S_{t-1}=s, A_{t-1}=a, S_t = s'] = \sum_{r\in\mathcal{R}}r \frac{p(s',r|s,a)}{p(s'|s,a)}$$

## 奖励与回报

在强化学习中，智能体的目标是最大化累积奖励，我们将累积奖励定义为回报，在最简单的形式下，

$$ G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_T \$$

但上面这种回报只适合分幕式任务，即存在自然终止状态的。对于连续性任务就不适用了。一般对于连续性任务需要加一个折损系数，即越将来的折现越小。

$$ G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$$

上式可以写成递归形式：

$$ G_t \doteq R_{t+1} + \gamma G_{t+1}$$

两式可以被统一成一个：

$$ G_t \doteq \sum_{k=t+1}^T \gamma^{k-t-1}R_k$$

## 策略与价值函数

几乎所有强化学习算法都涉及到价值函数的评估。价值函数衡量是某个状态下或者某个状态动作对下的期望回报。而价值函数又跟特定的决策动作序列相关，对应的决策动作分布被称为策略。策略$\pi(a|s)$可以被看作状态到动作的概率分布。

状态价值函数的定义可以给出：

$$ v_\pi(s) \doteq \mathbb{E}_\pi[G_t | S_t = s] = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg| S_t = s\right] $$

动作价值函数的定义类似：

$$ q_\pi(s,a) \doteq \mathbb{E}_\pi[G_t | S_t = s, A_t = a] = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1} \bigg| S_t = s, A_t = a\right] $$

状态价值函数和动作价值函数可以从经验中估算。只需要维护在特定策略下每个状态的回报平均值，根据大数定律，最终这个平均值会收敛到期望上。这也是蒙特卡洛方法的主要思想。但是当状态非常多的时候，这种方式可能无法落地，这时候我们就需要通过参数函数去拟合，这时候的效果与这个函数选择也相关。深度学习就落到了这个范畴。

对应于回报函数可以递归，状态价值函数也可以写成递归形式，即贝尔曼方程。

$$
\begin{align*}
 v_\pi(s) &\doteq \mathbb{E}_\pi[G_t | S_t = s] \\
 &= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
 &= \sum_a \pi(a|s)\sum_{s'}\sum_r p(s',r|s,a)[r + \gamma \mathbb{E}_\pi[G_{t+1}|S_{t+1}=s']] \\
 &= \sum_a \pi(a|s)\sum_{s',r} p(s',r|s,a)[r + \gamma v_\pi(s')]
\end{align*}
$$

对于动作价值函数，可以得到类似的贝尔曼方程。

$$
\begin{align*}
q_\pi(s,a) &\doteq \mathbb{E}_\pi[G_t | S_t = s, A_t = a] \\
&= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] \\
&= \sum_{s'}\sum_r p(s',r|s,a)[r + \gamma \sum_{a'}\pi(a'|s')\mathbb{E}_\pi[G_{t+1}|S_{t+1}=s', A_{t+1}=a']] \\
&= \sum_{s',r} p(s',r|s,a)[r + \gamma \sum_{a'}\pi(a'|s')q_\pi(s',a')]
\end{align*}
$$

## 最优策略和最优价值函数

如果一个策略在任一状态下都比其他策略的期望价值高，我们认为这就是一个最优策略。定义如下，对所有$s \in \mathcal{S}$：

$$ v_\ast(s) \doteq \max_\pi v_\pi(s)$$

最优策略可能有多个，但最优价值函数只有一个（否则就不满足最优策略的定义了）。

对应的，也有一个最优动作价值函数。

$$q_\ast(s,a) \doteq \max_\pi q_\pi(s,a) $$

他们存在如下关系：

$$q_\ast(s,a) = \mathbb{E}[R_{t+1} + \gamma v_\ast(S_{t+1}) | S_t=s, A_t=a] $$

根据最优策略的性质可以推导出贝尔曼最优方程：

$$
\begin{align*}
v_\ast(s) &= \max_{a \in \mathcal{A}(s)} q_{\pi_\ast}(s, a) \\
&= \max_a \mathbb{E}_{\pi_\ast}[G_t | S_t=s, A_t=a] \\
&= \max_a \mathbb{E}_{\pi_\ast}[R_{t+1} + \gamma G_{t+1} | S_t=s, A_t=a] \\
&= \max_a \mathbb{E}[R_{t+1} + \gamma v_\ast(S_{t+1}) | S_t=s, A_t=a] \\
&= \max_a \sum_{s',r} p(s',r|s,a)[r + \gamma v_\ast(s')]
\end{align*}
$$

类似的可以得到动作价值函数的贝尔曼最优方程：

$$
\begin{align*}
q_\ast(s,a) &= \mathbb{E}[R_{t+1} + \gamma \max_{a'}q_\ast(S_{t+1}, a')|S_t=s, A_t=a] \\
&= \sum_{s',r} p(s',r|s,a)[r + \gamma \max_{a'}q_\ast(s',a')]
\end{align*}
$$

从数学上论证，贝尔曼最优方程有唯一解。相当于针对每个状态有一个等式。如果有$|\mathcal{S}|$个状态就有$|\mathcal{S}|$个等式。当动力学方程已知的情况下，我们可以用任意方法解决非线性方程系统。

从最优状态价值函数推导出最优策略只需要做一步搜索，即在贝尔曼最优方程中找到能够让其最大的那个动作。而对最优动作价值函数而言则更加方便，对应的动作就是对应状态下最好的动作。

虽然贝尔曼方程存在理论解，但很少能够直接计算得到。直接计算依赖如下三个条件：

1. 准确的知道环境的动力学函数
2. 有足够的计算资源
3. 满足马尔可夫性质

相反，许多强化学习方法都是一种对应的近似解法。

## 近似最优解

除了上面的条件，在实际中内存也是一个重要的限制。如果任务状态集足够小，可以用表格形式在内存中存储近似价值函数，这种方式被称为表格法。但更多实际的例子里，价值函数无法存放进内存，这时候就只能用函数去拟合了。

近似可能也会带来未曾设想的好处。比如对低概率状态的次优动作不会特别影响整体的回报。而在线学习的性质恰好会让高概率的计算更准确。因此计算量是可以节省的，同时误差不会太大。