# 信息检索中的评估方法

信息检索是一类相对复杂的任务，传统的场景通常是用户给出一个查询，检索系统给用户返回一个经过排序的文档列表。大模型时代的生成式搜索场景下，用户被大模型所替代，经过排序的文档列表直接送给大模型做后续处理。针对这两个场景，我们来回顾一下信息检索中经典的评估方法。参考[此列表](https://ir-measur.es/en/latest/measures.html)。

## P 和 R

准确率和召回率都属于基础评估方法之一。准确率衡量召回文档中相关文档的比例，召回率衡量整体相关文档中被召回的比例。

四象限法可以比较直观的看出区别

| | 相关 | 不相关 |
|-- | -- | ---   |
| 召回 | 真正例tp | 伪正例fp |
| 不召回 | 假负例fn | 真负例tn |

那么可以得到 

$$
P = \frac{tp}{tp + fp} \\
R = \frac{tp}{tp + fn}
$$

需要注意到的是准确率和召回率是会冲突的，一般来说，随着召回数量的增加，召回率呈上升趋势，而准确率在超过一定阈值后呈下降趋势。另外有一个F指标是P和R的综合考量，在IR中用的比较少，所以不再提。

在实际使用中，往往会跟topK截断来使用。要注意的是，使用topK截断的准确率是不稳定的，因为库中相关doc的比例会影响实现这个准确率的难度。

准确率一般用在精排重排阶段，K取值较小。召回率一般衡量召回的效果。因此K取的就是召回的数量。另外注意在不同的搜索场景下两个指标也有侧重。网页搜索更注重准确率，而本地文件搜索更注重召回率。

## MAP

MAP是早期比较流行的一种评估方法，其也是从P和R中推导出来的。论公式，MAP比较复杂，但实际上其近似的就是P-R曲线下面的面积。对于检索需求$q_j \in Q$，相关的doc集合是$\{d_1,\cdots d_{m_j}\}$，而$R_{jk}$是直到获取到相关doc $d_k$时的top文档集合，我们可以得到，

$$
MAP(Q) = \frac{1}{|Q|} \sum_{j=1}^{|Q|} \frac{1}{m_j} \sum_{k=1}^{m_j} Precision(R_{jk})
$$

如果某个有效文档就没有被召回，那就是0。

AP部分的计算举个例子会更清洗，如果某个query总共有三个相关文档，分别排在1，4，9，那么$ AP = \frac{1}{1} + \frac{2}{4} + \frac{3}{9} $。

## NDCG

有了MAP作为底子，可以更容易的理解NDCG。对于有多级分类的标签而言，需要将多级标签的相关性考虑在内。同时，NDCG一般也是看截断topK的结果。

$$
NDCG(Q, k) = \frac{1}{|Q|} \sum_{j=1}^{|Q|} Z_k \sum_{m=1}^k \frac{2^{R(j,m)}-1}{\log(1+m)}
$$

这里NDCG计算时分子$2^{R(j,m)}-1$考虑了不同标签的价值，分母$\log(1+m)$又考虑了位置的折损。如果召回数量不足k时，计算到召回数量即可（后续项都为0）。

这里的$Z_k$是归一化系数，考虑的是最有排序结果IDCG，即以标签大小降序排列下的DCG。

## MRR

MRR是一种简单的针对binary标签的评估方法。其基于的假设是用户会自上而下寻找直到找到第一个相关文档位置为止。

$$
MRR = \frac{1}{|Q|} \sum_{j=1}^{|Q|} \frac{1}{R_j}
$$


## 二分类与多级标签

二分类标签胜在简单，人工标注的一致性高，容易为下游任务使用。多级标签胜在精细，更能体现IR系统的能力，也更贴近用户的评价。

在我的理解下，对于最终用户是人的场景，多级标签是更合适的，而对于最终用户是大模型的场景，二分类标签更有用。因为大模型最终可能就是选用相关的几条。



