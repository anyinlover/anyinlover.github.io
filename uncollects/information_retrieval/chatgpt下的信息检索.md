# ChatGPT下的信息检索

## 未来是否还需要传统的信息检索系统

在ChatGPT下体验了100个query查询，我认为传统信息检索系统当下仍然有需要的价值。

1. 当前的模型权威性无法保证。我怎么保证它没有对我一本正经的胡说八道？并且在可预见的将来这个问题也无法解决，尤其在法律，医学领域即便得分90分位，还是受概率支配。
2. 当前的模型的时效性无法保证。无法查询最新的新闻。这个问题未来可能会被解决。
3. 当前的模型无法查询部分长尾知识。比如查询扎兰屯疾病预防控制中心电话，如果训练语料里就没有包含这个知识，怎么能让大模型自动生成呢？如果大模型的训练语料里包含了会怎么样，大概率是可以吐出来的。但这个涉及爬取，清洗等各种环节，无法保证过程中这个知识没有丢失。
4. 响应速度和成本。当前的大模型响应速度较慢，推理成本过高。可能也会成为被广泛采纳的障碍。未来一定会改进。

从当前的体验来看，ChatGPT主要擅长知识百科类创作类推理类查询，分析日常查询：

长查询：可能会占据60%以上查询，如果时效性问题被搞定，预估会占据80%以上查询。（这些数据待实测）
短查询：在ChatGPT下，短查询已经不存在了，用户会用自然语言清晰的咨询问题。甚至寻址都能被ChatGPT搞定。

结论：

1. 信息检索系统仍然有存在的必要，但其重要性会降低。
2. 未来信息检索系统可能会退化成一个专业工具，而ChatGPT类工具将像微信一样占据日常使用场景。


## 如何在当前的信息检索系统中利用大模型

### NLU模块

NLU已经被大模型干掉了。

### 排序模块

权威性，时效性这些问题仍然是开放性问题，不是语言大模型可以搞定的。

相关性问题会被颠覆，其中深交互模型是大模型发力的点。如何让语义模型跳出title表征这种不完全信息下的排序，让正文（或者精简后的摘要）参与到在线排序中，将成为关键点。

大模型下的语义排序与之前的bert排序有什么区别？

1. 引入更大的模型，之前的bert系列是Encoder，如果模型更大，需要引入T5这类encoder-decoder，GPT这类decoder。模型越大，能力越强。
2. 加入Instruct Learning，[FLAN-T5能力远超T5](https://arxiv.org/abs/2210.11416)。
3. 更大的context长度，bert和T5的最大长度是512，GPT3的最大长度是2048，GPT4的最大长度是32768。可以有效看完文档正文。
4. 多模能力，GPT4已经支持图像输入了，大模型集成多模能力即将成熟。网页本身就是多模态的，天然适合多模输入。
5. 更强的在线推理性能。以上这些如果不能把推理时延打下去那就永远无法上线。GPT4一定是比GPT3更大的，但是推理性能可以做到类似。这里面有许多系统性的优化，包括百度发布会上王海峰也提到这一点。
6. 蒸馏剪枝等技术，这些技术在bert时代就存在，但是到大模型时代会更加突出。如何模型变小而效果不怎么下降？

### 召回模块

即使排序用上了大模型效果达到顶尖，如果召回没有跟上，仍然无法彻底解决长尾下的召回问题。

我认为长尾的召回问题还得靠SPLADE+倒排索引解决，无论用什么大模型，语义表征都无法在正文上实现，信息压缩比过高，而这始终是长尾问题的痛点。基于大模型的SPLADE词扩展，可以相对有效解决这类问题。

## 未来ChatGPT和检索系统的配合形式

1. NewBing模式，先检索再生成。
2. 先生成，用户按需求点击检索了解出处。
3. 自动判断query严肃性，常识类自动生成，严肃类查询先检索再生成。
4. 两列式，分别出生成内容和检索内容，相互独立。
