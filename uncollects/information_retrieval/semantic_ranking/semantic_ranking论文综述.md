# neural-reranking论文综述

## 模型结构

## 损失函数

## 样本优化

## 模型蒸馏

## 预训练模型

## 长文本处理

## 参考文献

Khattab, Omar, and Matei Zaharia. “ColBERT.” Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2020, https://doi.org/10.1145/3397271.3401075.

Lu, Wenhao, et al. “TwinBERT.” Proceedings of the 29th ACM International Conference on Information & Knowledge Management, Oct. 2020, https://doi.org/10.1145/3340531.3412747.

Nogueira, Rodrigo, and Kyunghyun Cho. “Passage Re-Ranking with BERT.” ArXiv:1901.04085 [Cs], Apr. 2020, arxiv.org/abs/1901.04085.

Lester, Brian, et al. “The Power of Scale for Parameter-Efficient Prompt Tuning.” ArXiv:2104.08691 [Cs], Sept. 2021, arxiv.org/abs/2104.08691.

Li, Xiang Lisa, and Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Generation.” ArXiv:2101.00190 [Cs], Jan. 2021, arxiv.org/abs/2101.00190.

Hu, Edward J., et al. “LoRA: Low-Rank Adaptation of Large Language Models.” ArXiv:2106.09685 [Cs], Oct. 2021, arxiv.org/abs/2106.09685.

Craswell, Nick, et al. “MS MARCO: Benchmarking Ranking Models in the Large-Data Regime.” Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2021, https://doi.org/10.1145/3404835.3462804.

