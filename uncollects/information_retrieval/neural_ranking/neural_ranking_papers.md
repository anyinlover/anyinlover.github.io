# 神经网络排序papers

[Neural ranking models for document retrieval](https://arxiv.org/abs/2102.11903)
[Learning Groupwise Multivariate Scoring Functions Using Deep Neural Networks](https://arxiv.org/abs/1811.04415)
[SetRank: Learning a Permutation-Invariant Ranking Model for Information Retrieval](https://www.semanticscholar.org/paper/SetRank%3A-Learning-a-Permutation-Invariant-Ranking-Pang-Xu/4697e5bd734131c0898d0dfe9351dd0ecc0c2c56)
[A Stochastic Treatment of Learning to Rank Scoring Functions](https://dl.acm.org/doi/10.1145/3336191.3371844)
[The LambdaLoss Framework for Ranking Metric Optimization](https://research.google/pubs/pub47258/)
[ARE NEURAL RANKERS STILL OUTPERFORMED BY GRADIENT BOOSTED DECISION TREES?](https://research.google/pubs/pub50030/)

TF-Ranking

[Advances in TF-Ranking](https://ai.googleblog.com/2021/07/advances-in-tf-ranking.html)
[TF-Ranking: A Scalable TensorFlow Library for Learning-to-Rank](https://ai.googleblog.com/2018/12/tf-ranking-scalable-tensorflow-library.html)

当前主要的神经网络排序模型：

1. GSF：a neural model using groupwise scoring function and fully connected layers
2. ApproxNDCG：a neural model with fully connected layers and a differeiable loss that approximates NDCG
3. DLCM(re)：an RNN based neural model that use list context information to rerank a list of documents based on λMARTRankLib
4. SetRank：a neural model using self-attention to encode the entire list and perform a joint scoring
5. SetRank(re)：SetRank plus ordinal embeddings based on the initial document ranking generated by λMARTRankLib
6. DASALC：Data Augmented SelfAttentive Latent Cross ranking network

神经网络LTR模型的三个主要问题：

1. 特征转换，LTR数据集的特征分布经常存在长尾，神经网络对此处理没有树模型好。
2. 神经网络架构，全连接网络不易学习到高维特征。
3. 训练数据量小，学习的模型深度不足。

当前主要的语义排序模型：

1. DSSM：a deep neural network model with fully connected layers that extracts features from query and document independently.
2. C-DSSM: a deep neural network model with cnn that extracts features from query and document independently.
