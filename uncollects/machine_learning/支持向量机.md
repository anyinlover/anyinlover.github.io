---
tags:
  - 机器学习
---

# 支持向量机

这一章主要学习支持向量机。支持向量机是目前最好的监督学习算法之一。但支持向量机本身难度较大，需要较强的数学基础。假如抛开数学部分，其本质就是用拉格朗日对偶法计算最大间隔，利用核函数简化高维映射的计算，用 SMO 算法更新参数。本文也将根据 NG 讲义的这一顺序展开。此外，[这里](http://blog.csdn.net/macyang/article/details/38782399)有一份比较完善的中文入门资料。

## 间隔入门

回顾之前的逻辑回归，$h_\theta(x)=g(\theta^Tx)$，当$\theta^Tx \geq 0$时预测为 1，否则为 0。这里存在一个预测的可信度问题，假如$\theta^Tx \gg 0$或者$\theta^Tx \ll 0$，那么预测的可信度就高。换一个角度，预测点离分割面越远，预测可信度就越高。把训练集离分割面最近的点到分割面的距离称为间隔，支持向量机的使命就是找到有最大间隔得分割面，使预测可信度提到最高。

## 标记

为了讨论支持向量机，需要引入一套新的标记符号。$y\in \{-1,1\}$而不再是$\{0,1\}$。分类器的表示也发生了变化：

$$h_{\omega,b}(x)=g(\omega^Tx+b)$$

这里用的 g 是感知机算法，即$g(z)=1,z\geq0$，否则取-1。b 是单独的截距，即之前的$\theta_0$。

## 函数间隔和几何间隔

根据一个训练样本$(x^{(i)},y^{(i)})$，定义函数间隔如下：

$$\hat{\gamma}^{(i)}=y^{(i)}(\omega^Tx+b)$$

函数间隔越大，可信度就越高。

给定训练集$S=\{(x^{(i)},y^{(i)});i=1,\cdots,m\}$，定义函数间隔是所有函数间隔中最小的那个：

$$\hat{\gamma}=\min_{i=1,\cdots,m}\hat{\gamma}^{(i)}$$

下面的问题是如何找出$\gamma^{(i)}$的表达式，分割面与$\omega$永远是正交的。对于训练样本在分割面上的投影，可以表示成

$$x^{(i)}-\gamma^{(i)} \cdot \frac{\omega}{\|\omega\|}$$

又因为投影点在分割面上，所以有：

$$\omega^T(x^{(i)}-\gamma^{(i)}\frac{\omega}{\|\omega\|})+b=0$$

解得：

$$\gamma^{(i)}=\frac{\omega^Tx^{(i)}+b}{\|\omega\|}=(\frac{\omega}{\|\omega\|})^Tx^{(i)}+\frac{b}{\|\omega\|}$$

结合 y 的取值可以得到几何间隔：

$$\gamma^{(i)}=y^{(i)}((\frac{\omega}{\|\omega\|})^Tx^{(i)}+\frac{b}{\|\omega\|})$$

我们发现几何间隔与函数间隔相比只是除了$\|\omega\|$，当$\|\omega\|=1$时函数间隔等于几何间隔。而且几何间隔不会受到比例系数的影响。

## 最优间隔分类器

对于一个可以线性分割的训练集来说，现在我们的任务就是要找到一个最大几何间隔。问题就转化成下面的优化问题：

$$
\begin{align}
\max_{\gamma,\omega,b}  \quad  &\gamma \\
\text{s.t.}  \quad & y^{(i)}(\omega^Tx^{(i)}+b)\geq \gamma, i=1,\cdots,m \\
& \|\omega\|=1
\end{align}
$$

对于$\|\omega\|=1$这样的约束条件很难处理，把几何间隔用函数间隔替换了，优化问题转化为：

$$
\begin{align}
\max_{\hat{\gamma},\omega,b}  \quad  & \frac{\hat{\gamma}}{\|\omega\|} \\
\text{s.t.}  \quad & y^{(i)}(\omega^Tx^{(i)}+b)\geq \hat{\gamma}, i=1,\cdots,m
\end{align}
$$

进一步，利用$\hat{\gamma}$的可伸缩性质，令$\hat{\gamma}=1$，求$1/\|\omega\|$相当于求$\|\omega\|^2$的最小值。

$$
\begin{align}
\min_{\omega,b}  \quad  & \frac{1}{2}\|\omega\|^2 \\
\text{s.t.}  \quad & y^{(i)}(\omega^Tx^{(i)}+b)\geq 1, i=1,\cdots,m
\end{align}
$$

现在优化问题转变成了一个凸二次函数带一个线性约束，这样的优化问题可以用 QP 软件来处理。

## 拉格朗日对偶

我们到了支持向量机的难点之一，拉格朗日对偶。这其实就是一种把多元约束优化问题转换为一元约束优化问题的思想。
考虑下面这样的问题形式：

$$
\begin{align}
\min_\omega \quad & f(\omega) \\
\text{s.t.} \quad & h_i(\omega)=0, i=1,\cdots,l
\end{align}
$$

我们定义拉格朗日公式为：

$$\mathcal{L}(\omega,\beta)=f(\omega)+\sum_{i=1}^l \beta_ih_i(\omega)$$

这里$\beta_i$被称为拉格朗日算子。通过求偏导可以求解得到$\omega$和$\beta$：

$$\frac{\partial\mathcal{L}}{\partial\omega_i}=0; \frac{\partial \mathcal{L}}{\partial\beta_i}=0$$

将上面的形式进行拓展，把不等约束也包含在内，考虑下面的问题形式，称为原始优化问题：

$$
\begin{align}
\min_\omega \quad & f(\omega) \\
\text{s.t.} \quad & g_i(\omega) \leq 0, i=1,\cdots,k \\
& h_i(\omega)=0, i=1,\cdots,l
\end{align}
$$

为了解决上述问题，我们定义了一般化的拉格朗日方程：

$$\mathcal{L}(\omega,\alpha,\beta)=f(\omega)+\sum_{i=1}^k\alpha_ig_i(\omega)+\sum_{i=1}^l\beta_ih_i(\omega)$$

考虑以下方程：

$$\theta_{\mathcal{p}}(\omega)=\max_{\alpha,\beta:\alpha_i \geq 0}\mathcal{L}(\omega,\alpha,\beta)$$

假如$g_i(\omega)>0$ 或者$h_i(\omega) \neq 0$，都会使得$\theta_{\mathcal{p}}(\omega)=\infty$

$$
\theta_{\mathcal{p}}(\omega)=
\begin{cases}
f(\omega) \quad & \text{if }\omega \text{ satisfies primal constraints}\\
\infty \quad & \text{otherwise}
\end{cases}
$$

考虑最小化问题：

$$\min_{\omega} \theta_{\mathcal{p}}(\omega)=\min_{\omega} \max_{\alpha,\beta:\alpha_i \geq 0} \mathcal{L}(\omega,\alpha,\beta)$$

这就是我们的原始优化问题。最后定义优化值$p^*=\min_{\omega} \theta_p(\omega)$，称为原始问题的值。

再来看一个有些不同的问题。定义：

$$\theta_{\mathcal{D}}(\alpha,\beta)=\min_{\omega} \mathcal{L}(\omega,\alpha,\beta)$$

我们可以得到对偶优化问题：

$$\max_{\alpha,\beta:\alpha_i \geq 0} \theta_{\mathcal{D}}(\alpha,\beta)=\max_{\alpha,\beta:\alpha_i \geq 0} \min_{\omega} \mathcal{L}(\omega,\alpha,\beta)$$

对偶优化问题相当于和初始优化问题交换了 max 和 min 的顺序，我们定义对偶优化问题的优化值为$d^*=\max_{\alpha,\beta:\alpha_i \geq 0} \theta_{\mathcal{D}} (\omega)$

关于 max 和 min 有下面的结论：

$$\max_x \min_y f(x,y) \leq \min_y \max_x f(x,y)$$

简要证明：对于任意 x,y，下式成立：

$$\min_s f(x,s) \leq f(x,y) \leq\max_tf(t,y)$$

因此下式成立，得到证明：

$$ \max_x \min_s f(x,s) \leq \min_y \max_t f(t,y)$$

回到我们的拉格朗日对偶问题：

$$d^*=\max_{\alpha,\beta:\alpha_i \geq 0} \min_{\omega} \mathcal{L}(\omega,\alpha,\beta) \leq \min_{\omega} \max_{\alpha,\beta:\alpha_i \geq 0} \mathcal{L}(\omega,\alpha,\beta) = p^*$$

在满足一定的条件下，上式中等号成立，因此我们可以用求解对偶问题来求解原始问题。

假设$f$和$g_i$是凸函数，$h_i$是线性的，约束$g_i$是可满足的。那必然会存在$\omega^*, \alpha^*, \beta^*$满足$p^*=d^*=\mathcal{L}(\omega^*,\alpha^*,\beta^*)$，他们满足下面的 KKT 条件：

$$
\begin{align}
\frac{\partial}{\partial \omega_i}\mathcal{L}(\omega^*, \alpha^*, \beta^*) &= 0, i=1,\cdots,n \\
\frac{\partial}{\partial \beta_i}\mathcal{L}(\omega^*, \alpha^*, \beta^*) &= 0, i=1,\cdots,l \\
\alpha_i^*g_i(\omega^*) &= 0, i=1,\cdots,k \\
g_i(\omega^*) & \leq 0, i=1,\cdots,k \\
\alpha^* & \geq 0, i=1,\cdots,k
\end{align}
$$

相反的，假如存在$\omega^*, \alpha^*, \beta^*$满足 KKT 条件，那它也是拉格朗日问题的解。

在 KKT 条件的第三式中我们还发现，必须满足$\alpha_i^*>0$当$g_i(\omega^*) = 0$。

（本来以为拉格朗日对偶会很困难，啃下来发现更多的是心理作用，233）

## 优化间隔分类器

回到我们的优化间隔分类器，我们可以把约束写成标准形式：

$$g_i(\omega)=-y^{(i)}(\omega^Tx^{(i)}+b)+1 \leq 0$$

根据 KKT 条件，只有在函数间隔是 1 时才能$\alpha_i^*>0$。实际上只有很少量的点决定了最大间隔。这些点被称为支持向量。

把优化问题写成下面的拉格朗日形式，注意我们这里不带$\beta_i$：

$$\mathcal{L}(\omega,b,\alpha)=\frac{1}{2}\|\omega\|^2-\sum_{i=1}^m \alpha_i[y^{(i)}(\omega^Tx^{(i)}+b)-1]$$

我们首先找出拉格朗日对偶问题，即先固定$\alpha$，求对于$\omega,b$的最小值。分别求$\mathcal{L}$对$\omega,b$的偏导：

$$\nabla_{\omega} \mathcal{L}(\omega,b,\alpha)=\omega - \sum_{i=1}^m\alpha_iy^{(i)}x^{(i)}=0$$

由此得到：

$$\omega=\sum_{i=1}^m \alpha_iy^{(i)}x^{(i)}$$

对 b 求偏导，我们得到：

$$\frac{\partial}{\partial b} \mathcal{L}(\omega, b, \alpha)=\sum_{i=1}^m \alpha_iy^{(i)}=0$$

将$\omega,b$代回$\mathcal{L}$表达式，我们得到：

$$\mathcal{L}(\omega,b,\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j(x^{(i)})^Tx^{(j)}$$

最终，我们得到下面的拉格朗日对偶问题：

$$
\begin{align}
\max_{\alpha} \quad & W(\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j\langle x^{(i)},x^{(j)} \rangle \\
s.t. \quad & \alpha_i \geq 0, i=1,\cdots,m \\
& \sum_{i=1}^m \alpha_iy^{(i)}=0
\end{align}
$$

因为我们满足 KKT 条件，因此可以通过解答拉格朗日对偶问题来解决原始问题。对偶问题需要用下面的核函数解决。

在得到$\alpha^*$后，也就能得到$\omega^*$，$b^*$可以通过下式得到：

$$b^*=-\frac{\max_{i:y^{(i)}=-1} \omega^{*T}x^{(i)}+\min_{i:y^{(i)}=1}\omega^{*T}x^{(i)}}{2}$$

进一步讨论一下预测问题，对于新输入的 x，我们要计算$\omega^Tx+b$：

$$\omega^Tx+b=(\sum_{i=1}^m \alpha_iy^{(i)}x^{(i)})^Tx+b=\sum_{i=1}^m \alpha_iy^{(i)}\langle x^{(i)}, x \rangle + b$$

我们注意到预测时也只用到了 x 的内积，对偶问题中同样只用了内积。此外，只有支持向量的$\alpha_i$不为 0，因此这一步运算花费很少。

## 核函数

在房价预测中，我们用$x,x^2,x^3$来得到一个三次方函数。这里我们把原始$x$称为属性，把$x,x^2,x^3$称为特征。用$\phi$记为特征映射。在我们的例子里，有：

$$\phi(x)=\begin{bmatrix}x \\ x^2 \\ x^3\end{bmatrix}$$

将内积$\langle x,z\rangle$用$\langle \phi(x), \phi(z) \rangle$代替，定义对应的核函数为：

$$K(x,z)=\phi(x)^T \phi(z)$$

巧妙的地方在于可能$\phi(x)$的计算量很大，但$K(x,z)$却很容易。实际在算法中我们只需要应用到$K(x,z)$，却不必知道$\phi(x)$是多少。举下面两个例子。

考虑核函数$K(x,z)=(x^Tz)^2$：

$$
\begin{align}
K(x,z) &= (\sum_{i=1}^n x_iz_i)(\sum_{j=1}^n x_iz_i) \\
&= \sum_{i=1}^n \sum_{j=1}^n x_i x_j z_i z_j \\
&= \sum_{i,j=1}^n (x_ix_j)(z_iz_j)
\end{align}
$$

因此特征映射函数为下式，需要$O(n^2)$时间，而核函数只需要$O(n)$：

$$\phi(x)=\begin{bmatrix}x_1x_1 \\ x_1x_2 \\ x_1x_3 \\ x_2x_1 \\ x_2x_2 \\ x_2x_3 \\ x_3x_1 \\ x_3x_2 \\ x_3x_3\end{bmatrix}$$

再考虑一个相关的核函数：

$$K(x,z)=(x^Tz+c)^2=\sum_{i,j=1}^n (x_ix_j)(z_iz_j)+ \sum_{i=1}^n( \sqrt{2c}x_i)(\sqrt{2c}z_i) + c^2$$

此时的特征函数为：

$$\phi(x)=\begin{bmatrix}x_1x_1 \\ x_1x_2 \\ x_1x_3 \\ x_2x_1 \\ x_2x_2 \\ x_2x_3 \\ x_3x_1 \\ x_3x_2 \\ x_3x_3 \\ \sqrt{2c}x_1 \\ \sqrt{2c}x_2 \\ \sqrt{2c}x_3 \\ c\end{bmatrix}$$

更一般的，核函数$K(x,z)=(x^Tz+c)^d$把特征映射到了$\binom{n+d}{d}$维特征空间。需要$O(n^d)$时间，而核函数仍只需要$O(n)$。

从另一个角度粗略的讲，$K(x,z)=\phi(x)^T \phi(z)$体现了$\phi(x)$和$\phi(z)$之间的接近关系，越接近核函数越大，越远离核函数越小。比如下面的高斯核函数，很好的衡量了 x 和 z 之间的关系：

$$K(x,z)=\exp(- \frac{\|x-z\|^2}{2\sigma^2})$$

但现在有一个问题，我怎么知道这个核函数是有意义的，即能找出特征映射$\phi$满足$K(x,z)=\phi(x)^T \phi(z)$。

假设 K 是一个有效的核函数，给定有限点集$\{x^{(1)},\cdots,x^{(m)}\}$，令一个核函数矩阵$K \in \mathbb{R}^{m \times m}, K_{ij}=K(x^{(i)},x^{(j)})$。

假如 K 是个有效核函数，则有：

$$K_{ij}=K(x^{(i)},x^{(j)})=\phi(x^{(i)})^T\phi(x^{(j)})=\phi(x^{(j)})^T\phi(x^{(i)})=K(x^{(j)},x^{(i)})=K_{ji}$$

还能证明核函数矩阵是半正定的：

$$
\begin{align}
z^TKz &= \sum_i\sum_jz_iK_{ij}z_j \\
&=\sum_i\sum_jz_i\phi(x^{(i)})^T\phi(x^{(j)})z_j \\
&=\sum_i\sum_jz_i\sum_k\phi_k(x^{(i)})\phi_k(x^{(j)})z_j\\
&=\sum_k\sum_i\sum_jz_i\phi_k(x^{(i)})\phi_k(x^{(j)})z_j \\
&= \sum_k(\sum_iz_i\phi_k(x^{(i)}))^2 \\
&\geq 0
\end{align}
$$

因此当核函数是有效的，核函数矩阵是对称半正定的。事实上这不仅仅是一个必要条件，还是一个充分条件。归纳为 Mercer 定理。

除了应用在支持向量机中，核函数还在其他算法中大量应用。这种应用被称为核方法。（个人粗浅的理解，核方法看起来像是小技巧，实际上体现事物的内在本质）。

## 规则化和不可分情况

之前推导支持向量机时我们假定数据是线性可分割的，通过特征映射我们也能完成非线性分割，但有时候仍然会发生训练结果不好的情况，因为数据中会有噪音或离群点影响。

为此，我们引入规则化和松弛变量，将优化问题转化为下面的问题：

$$
\begin{align}
\min_{\omega,b, \xi}  \quad  & \frac{1}{2}\|\omega\|^2 + C\sum_{i=1}^m \xi_i\\
\text{s.t.}  \quad & y^{(i)}(\omega^Tx^{(i)}+b)\geq 1-\xi_i, i=1,\cdots,m \\
& \xi_i \geq 0, i=1,\cdots,m
\end{align}
$$

构造拉格朗日方程：

$$\mathcal{L}(\omega,b,\xi,\alpha,r)=\frac{1}{2}\omega^T\omega+C\sum_{i=1}^m \xi_i - \sum_{i=1}^m \alpha_i[y^{(i)}(x^T\omega+b)-1+\xi_i]-\sum_{i=1}^m r_i\xi_i$$

采用对$\omega,b, \xi$求偏导的方法，可以得到对偶方程：

$$
\begin{align}
\max_{\alpha} \quad & W(\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j\langle x^{(i)},x^{(j)} \rangle \\
s.t. \quad & 0 \leq \alpha_i \leq C, i=1,\cdots,m \\
& \sum_{i=1}^m \alpha_iy^{(i)}=0
\end{align}
$$

我们注意到对偶方程和未引入松弛变量前几乎一致，除了$\alpha$取值范围有变化。因为

$$\frac{\partial}{\partial \xi_i} \mathcal{L}(\omega,b,\xi,\alpha,r)=C-\alpha_i-r_i=0$$

根据 KKT 条件，还能得出以下结果：

$$
\begin{align}
\alpha_i=0 &\Rightarrow y^{(i)}(\omega^Tx^{(i)}+b) \geq 1 \\
\alpha_i=C &\Rightarrow y^{(i)}(\omega^Tx^{(i)}+b) \leq 1 \\
0 \leq \alpha_i \leq C &\Rightarrow y^{(i)}(\omega^Tx^{(i)}+b) = 1
\end{align}
$$

## SMO 算法

SMO 算法专门用来高效解决支持向量机中推导出来的拉格朗日对偶问题。在讨论 SMO 算法之前先讲讲坐标上升法。

### 坐标上升法

坐标上升法是另一种优化方法，类似于前面的梯度下降法和牛顿法。其实质是每次只在一个坐标方向优化，考虑以下的无约束优化问题：

$$\max_{\alpha} W(\alpha_1, \alpha_2, \cdots, \alpha_m)$$

使用以下的算法进行递归直到达到最优解：

$$
\alpha_i := \arg \max_{\hat{\alpha_i}} W(\alpha_1,\cdots,\alpha_{i-1},\hat{\alpha_i},\alpha_{i+1},\cdots,\alpha_m)
$$

按顺序每一次固定其他，更新一个变量。（更复杂的版本是每一次更新使 W 增加最快的变量）。

当函数 W 是类似于 arg max 这样高效运算的函数时，坐标上升法是一种很有效的算法。

### SMO

我们回到 SMO 算法来解决拉格朗日对偶问题。
观察约束$\sum_{i=1}^m \alpha_i y^{(i)}=0$，我们发现$\alpha$并不完全独立，$\alpha_1=-y^{(1)}\sum_{i=2}^m\alpha_iy^{(i)}$。

受坐标上升法的启发，SMO 算法的核心就是选择一对参数（使 W 增长最快的两个）进行更新，直到优化结束。

假设选择$\alpha_1,\alpha_2$进行更新：

$$\alpha_1y^{(1)}+\alpha_2y^{(2)}=-\sum_{i=3}^m \alpha_iy^{(i)}=\zeta$$

结合$0\leq \alpha_i \leq C$的约束，$\alpha_2$的真实取值范围是$L \leq \alpha_2 \leq H$。最终更新结果是：

$$
\alpha_2^{new}=
\begin{cases}
H \quad &\text{if } \alpha_2^{new, unclipped} > H \\
\alpha_2^{new, unclipped} \quad &\text{if } L \leq \alpha_2^{new, unclipped} \leq H \\
L
\end{cases}
$$

$\alpha_1^{new}$值可以通过前面的线性关系求得。

SMO 这里讲的还是比较粗略，需要后续再补充。不过支持向量机到此终于完结了，花了三天半时间，也是值得。
