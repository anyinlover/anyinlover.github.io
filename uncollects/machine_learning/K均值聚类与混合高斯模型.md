---
tags:
  - 机器学习
---

# K均值聚类与混合高斯模型

## K 均值聚类算法

在聚类问题中，我们给定一个训练集$\{x^{(1)},\cdots,x^{(m)}\}$，标签$y^{(i)}$没有给出，我们的目标还是把数据分类。这是一个非监督学习问题。

K 均值聚类算法给出了下面的方法：

1. 随机初始化聚类中心$\mu_1,\mu_2,\cdots,\mu_k \in \mathbb{R}^n$
2. 不断迭代直到收敛：{

对所有 i，令：

$$c^{(i)} := \arg \min_j\|x^{(i)}-\mu_j\|^2$$

对所有 j，令：

$$\mu_j := \frac{\sum_{i=1}^m 1\{c^{(i)}=j\}x^{(i)}}{\sum_{i=1}^m 1\{c^{(i)}=j\}}$$

}

算法的内循环不停的在执行两个步骤，先把每个训练样本归给最近的聚类中心所代表的类，然后将某类的所有点计算平均值作为新的聚类中心。

现在有一个问题，如何保证 K 均值聚类算法一定会收敛？定义失真函数为：

$$J(c,\mu)=\sum_{i=1}^m\|x^{(i)}-\mu_{c^{(i)}}\|^2$$

失真函数定义了所有训练集离其聚类中心平方距离和。对于算法的内循环的两步而言，每一步都是朝着 J 下降的方向进行。实际上 K 均值聚类算法就是一类坐标下降法。最终 J 会达到收敛。

失真函数 J 是一个非凸函数，所以坐标下降法不能保证 J 能收敛到全局最小值。为了不限于局部最小值，可以多次跑 K 均值聚类算法（使用不同的随机初始值），取最低失真函数。

## 混合高斯模型

同样假设下面的非监督学习问题，给定一组训练集$\{x^{(i)},\cdots,x^{(m)}\}$，没有标签。

我们用一个联合分布$p(x^{(i)},z^{(i)})=p(x^{(i)} \mid z^{(i)})p(z^{(i)})$来对数据进行建模。这里，$z^{(i)} \sim \text{Multinomial}(\phi)（\phi_j \geq 0, \sum_{j=1}^k \phi_j = 1, \phi_j=p(z^{(i)}=j))$，$x^{(i)} \mid z^{(i)}=j \sim \mathcal{N}(\mu_j,\Sigma_j)$。因此我们的模型是随机从$\{1,\cdots,k\}$中选择$z^{(i)}$，然后从对应$z^{(i)}$的高斯分布中选择$x^{(i)}$。这被称为混合高斯模型。此外由于$z^{(i)}$是潜在的随机变量，这使我们的评估变得困难。

混合高斯模型的参数有$\phi,\mu,\Sigma$，最大似然函数是：

$$
\begin{align}
\ell(\phi,\mu,\Sigma) &= \sum_{i=1}^m \log p(x^{(i)};\phi,\mu,\Sigma) \\
&= \sum_{i=1}^m \log \sum_{z^{(i)}=1}^k p(x^{(i)}\mid z^{(i)};\mu,\Sigma)p(z^{(i)};\phi)
\end{align}
$$

然而上式无法用梯度来计算，因此也无法解得最大似然估计。

注意上式中$\sum_{z^{(i)}=1}^k$产生的原因在于$z^{(i)}$是未知的，假如$z^{(i)}$是已知的，那么最大似然函数可以简化为：

$$\ell(\phi,\mu,\Sigma)=\sum_{i=1}^m\log p(x^{(i)}\mid z^{(i)};\mu,\Sigma)+p(z^{(i)};\phi)$$

做最大似然估计可以解出：

$$
\begin{align}
\phi_j &= \frac{1}{m} \sum_{i=1}^m 1 \{z^{(i)}=j\}  \\
\mu_j &= \frac{\sum_{i=1}^m 1\{z^{(i)}=j\}x^{(i)}}{\sum_{i=1}^m 1\{z^{(i)}=j\}} \\
\Sigma_j &= \frac{\sum_{i=1}^m 1\{z^{(i)}=j\}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m 1\{z^{(i)}=j\}}
\end{align}
$$

事实上，假如$z^{(i)}$是已知的，那么最大似然估计和之前的高斯判别分析非常类似，只是$z^{(i)}$表示了标签。

然而$z^{(i)}$是未知的，这里就引出了最大期望算法。最大期望算法主要由两步构成，E 步尝试猜测$z^{(i)}$的值，M 步根据我们猜测的$z^{(i)}$更新模型参数。

迭代直到收敛：

{

（E 步）对每个 i,j，令：

$$\omega_j^{(i)} := p(z^{(i)}=j\mid x^{(i)}; \phi, \mu, \Sigma)$$

（M 步）更新参数：

$$
\begin{align}
\phi_j &:= \frac{1}{m} \sum_{i=1}^m \omega_j^{(i)}  \\
\mu_j &:= \frac{\sum_{i=1}^m \omega_j^{(i)}x^{(i)}}{\sum_{i=1}^m \omega_j^{(i)}} \\
\Sigma_j &= \frac{\sum_{i=1}^m \omega_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m \omega_j^{(i)}}
\end{align}
$$

}

在 E 步，我们通过计算后验概率来得到：

$$p(z^{(i)}=j\mid x^{(i)}; \phi, \mu, \Sigma)=\frac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}$$

EM 算法和 K 均值聚类算法有很多相似的地方，都是两步走，都是可能陷入局部最优解，需要多次不同的初始值进行计算。

EM 算法是否确保收敛等性质放到下一章证明。
