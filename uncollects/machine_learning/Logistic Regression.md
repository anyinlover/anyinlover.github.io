# Logistic Regression

In binary [[Classification]] problem, since $y$ can only take values 0 and 1, a natural approach is to map $X$ to the (0, 1) space. The logistic function([[Sigmoid Function]]) satisfactorily fulfills this property: it quickly tends to the boundaries on both sides. (Note that logistic is not the only choice.) The logistic regression model involves an additional logistic calculation on top of linear regression:

$$
h(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}
$$

Here is the log-likelihood function:

$$
\begin{aligned}
\ell(\theta)&=\log L(\theta)\\
&=\sum_{i=1}^n y^{(i)}\log h(x^{(i)})+(1-y^{(i)})\log (1-h(x^{(i)}))
\end{aligned}
$$

For a single training sample, taking the derivative of its log-likelihood function:

$$
\begin{aligned}
\frac{\partial}{\partial\theta_j}&=(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)})\frac{\partial}{\partial\theta_j}g(\theta^Tx) \\
&=(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)})g(\theta^Tx)(1-g(\theta^Tx))\frac{\partial}{\partial\theta_j}\theta^Tx\\
&=(y(1-g(\theta^Tx))-(1-y)g(\theta^Tx))x_j\\
&=(y-h_\theta(x))x_j
\end{aligned}
$$

Using [[Stochastic Gradient Descent]]:

$$
\theta_j:=\theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}
$$

Surprisingly, this equation is exactly the same as [[Linear Regression]]! Of course, here $h_\theta(x)$ is different, it can be proven by [[Generalized Linear Models]]


[//begin]: # "Autogenerated link references for markdown compatibility"
[Classification]: Classification.md "Classification"
[Sigmoid Function]: <Sigmoid Function.md> "Sigmoid Function"
[Stochastic Gradient Descent]: <Stochastic Gradient Descent.md> "Stochastic Gradient Descent"
[Linear Regression]: <Linear Regression.md> "Linear Regression"
[Generalized Linear Models]: <Generalized Linear Models.md> "Generalized Linear Models"
[//end]: # "Autogenerated link references"
