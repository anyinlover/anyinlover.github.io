# Classification

In classification problems, the output space is a set of C unordered and mutually exclusive labels known as classes, $\mathcal{Y} = \{1, 2, \dots, C\}$. (If there are just two classes, often denoted by $y \in \{0, 1\}$, it is called binary classification.)

The most common loss function for classification is [[Cross Entropy]], and the empirical risk when using cross entropy is equal:

$$
\text{Loss} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log(p_{i,c})
$$

For binary classification, it becomes:

$$
\text{Loss} = -\frac{1}{N}\sum_{i=1}^{N}[y_{i}\log(p_{i})+(1-y_{i})\log(1-p_{i})]
$$

It can be derived from [[Maximum Likelihood Estimate]].

For a classification problem, the model outputs a **probability distribution** over the possible classes. For a single training example, let:

- $\mathbf{y} = (y_1, y_2, ..., y_C)$ be the true label (a one-hot vector where the true class is 1 and the rest are 0).
- $\hat{\mathbf{p}} = (\hat{p}_1, \hat{p}_2, ..., \hat{p}_C)$ be the predicted probability distribution over the $C$ classes, i.e., $\hat{p}_c = P(y=c | \mathbf{x})$, where $\mathbf{x}$ is the input.

The likelihood of the true label given the predicted probabilities is the product of the predicted probabilities for the true class:

$$
P(\mathbf{y} | \hat{\mathbf{p}}) = \hat{p}_t
$$

where $t$ is the true class index, and $\hat{p}_t$ is the predicted probability for the true class.

The **log-likelihood** of the true label is:

$$
\log P(\mathbf{y} | \hat{\mathbf{p}}) = \log \hat{p}_t
$$

The negative log-likelihood loss for $N$ examples is:

$$
\mathcal{NLL} = - \frac{1}{N} \sum_{i=1}^{N} \log \hat{p}_{t_i} = \text{Loss}
$$


[//begin]: # "Autogenerated link references for markdown compatibility"
[Cross Entropy]: <Cross Entropy.md> "Cross Entropy"
[Maximum Likelihood Estimate]: <Maximum Likelihood Estimate.md> "Maximum Likelihood Estimate"
[//end]: # "Autogenerated link references"
