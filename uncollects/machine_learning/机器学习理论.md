---
tags:
  - 机器学习
---

# 机器学习理论

## 偏差和方差

还是从房价预测的例子入手，我们可以分别用一次函数，三次函数和五次函数去拟合测试集。但不是三个模型都是好模型，第一个和第三个都存在着泛化误差，即测试集的拟合误差要大于训练集的期望误差。

对于一次函数模型，我们认为模型有偏差，即使给出足够大的训练集也会使数据欠拟合。而对于五次函数模型，我们认为模型有方差，仅仅是很好的吻合了小范围的测试集，对数据过拟合，不能准确反映更一般的输入输出关系。

很多时候都需要在偏差和方差之间做权衡。假如我们的模型过于简单只有很少的几个参数，那很可能有大的偏差；假如模型过于复杂有很多的参数，那可能会有大的方差。

## 预备知识

我们开始学习一些机器学习原理中最基石的规则。最终希望能够回答三个问题：

1. 我们是否可以把偏差和方差公式化？最终引出模型选择方法。
2. 为何从测试集中可以获得泛化误差？测试集误差和泛化误差是否有联系？
3. 是否能在某些条件下证明算法一定有效？

首先介绍两个引理（这里没有证明，可以直观感受）。

**联合界定理**：存在$A_1,A_2,\cdots,A_k$共 k 个不同事件（可能独立也可能非独立），必然有：

$$P(A_1 \cup \cdots \cup A_k) \leq P(A_1)+\cdots + P(A_k)$$

**霍夫丁不等式**：令$Z_1,\cdots,Z_m$是 m 个服从伯努利分布的独立同分布随机变量，令$\hat{\phi}=(1/m)\sum_{i=1}^m Z_i$作为随机变量的均值，令任意$\gamma>0$固定，有下面的关系：

$$P(|\phi - \hat{\phi}| > \gamma) \leq 2\exp(-2\gamma^2m)$$

为了简化解释，我们再以二元分类为例。假设给定一个训练集$S=\{(x^{(i)},y^{(i)};i=1,\cdots,m\}$，训练样本$(x^{(i)},y^{(i)})$满足可能性分布$\mathcal{D}$，对于假说 h，定义训练误差（经验误差）为：

$$\hat{\epsilon}(h)=\frac{1}{m} \sum_{i=1}^m1\{h(x^{(i)} \neq y^{(i)}\}$$

定义泛化误差为：

$$\epsilon(h)=P_{(x,y) \sim \mathcal{D}}(h(x) \neq y)$$

PAC 是一组构建机器学习原理的假定。其中最重要的两条就是训练集和测试集满足同分布，训练样本具备独立性。

考虑线性分类，令$h_\theta(x)=1\{\theta^Tx \geq 0\}$，评估参数$\theta$拟合的一种方式就是让训练误差最小化：

$$\hat{\theta} = \arg \min_{\theta} \hat{\epsilon}(h_{\theta})$$

这个过程被称为经验风险最小化，它被视为是最基础的学习算法。ERM 本身是非凸不能用一般优化算法求解的，逻辑回归和支持向量机被看做对这种算法的凸性近似。

更一般化，我们用假设集$\mathcal{H}$来定义一组分类器。比如对于线性分类法，$\mathcal{H}=\{h_{\theta}: h_{\theta}(x)=1\{\theta^Tx \geq 0\}, \theta \in \mathbb{R}^{n+1}\}$。经验风险最小化可以写成下式：

$$\hat{h}=\arg \min_{h \in \mathcal{H}} \hat{\epsilon}(h)$$

## 有限假设集

我们先来考虑假说集有限的情况，即$\mathcal{H}=\{h_1,\cdots,h_k\}$，假设集由 k 个假说构成。经验风险最小化算法选择其中使训练误差最小的假说作为$\hat{h}$。

考虑一个伯努利随机变量 Z，样本$(x,y) \sim \mathcal{D}$，$Z=1\{h_i(x) \neq y\}$。对训练样本我们同样定义$Z_j=1\{h_i(x^{(j)}) \neq y^{(j)}\}$。训练样本和测试样本服从同分布。

可以看到误分类的可能性$\epsilon(h)$就等于$Z(Z_j)$的期望值，此外，训练误差可表示成：

$$\hat{\epsilon}(h_i)=\frac{1}{m}\sum_{j=1}^m Z_j$$

因此我们在这里可以应用霍夫丁不等式：

$$P(|\epsilon(h_i)-\hat{\epsilon}(h_i)| > \gamma) \leq 2\exp(-2\gamma^2m)$$

这显示了当 m 足够大时，对特定的$h_i$训练误差有极高的可能性与泛化误差接近。下面我们要证明对于所有$h \in \mathcal{H}$，上面的特性也成立。

$$
\begin{aligned}
P(\exists h \in \mathcal{H}.|\epsilon(h_i)-\hat{\epsilon}(h_i)| > \gamma) &= P(A_1 \cup \cdots \cup A_k) \\
&\leq \sum_{i=1}^k P(A_i) \\
&\leq \sum_{i=1}^k 2 \exp(-2\gamma^2m) \\
&=2k \exp(-2\gamma^2m)
\end{aligned}
$$

这个结果被称为一致收敛，对于所有 h 都满足。

令$\delta=2k\exp(-2\gamma^2m)$，我们可以计算出为达到概率在$1-\delta$，精确度在$\pm\gamma$内要求所需的样本复杂度 m：

$$ m \geq \frac{1}{2\gamma^2} \log \frac{2k}{\delta}$$

同样，我们也可以求得精确度：

$$|\hat{\epsilon}(h)-\epsilon(h)| \leq \sqrt{\frac{1}{2m} \log{\frac{2k}{\delta}}}$$

现在还有一个问题，模型的泛化误差和最小训练误差$\hat{h}=\arg \min_{h \in \mathcal{H}} \hat{\epsilon}(h)$存在什么联系？

令$h^*=\arg \min_{h \in \mathcal{H}} \epsilon(h)$作为假设集$\mathcal{H}$中最好的一个，它与训练误差最小的假设存在以下关系：

$$
\begin{aligned}
\epsilon(\hat{h}) &\leq \hat{\epsilon}(\hat{h})+\gamma \\
&\leq \hat{\epsilon}(h^*)+\gamma \\
&\leq \epsilon(h^*)+2\gamma
\end{aligned}
$$

在$\mid \mathcal{H}\mid =k$，$m,\delta$固定，至少有$1-\delta$的可能性：

$$\epsilon(\hat{h}) \leq (\min_{h\in\mathcal{H}} \epsilon(h)) + 2 \sqrt{\frac{1}{2m} \log \frac{2k}{\delta}}$$

这也从另一面证明了偏差和方差的矛盾性。假设我们的假设集扩大了，则前一项下降，后一项增加。反之亦然。

## 无限假设集

在有限假设集中我们得出了一些有用的定理。但对于参数是实数的假设集来说，有无限个假设。我们是否能得出类似的结论？

首先做一个不是很正确的解释。因为实数在计算机中也是由有限位组成，因此所谓的无限假设实际上也是有限的，因此可以套用上一节的结论来处理。

为得出无限假设集的结果，我们需要定义 VC 维。给定样本点集合$S=\{x^{(i)},\cdots,x^{(d)}\}, x^{(i)} \in \mathcal{X}$，我们说$\mathcal{H}$粉碎$S$假如$\mathcal{H}$可以识别任意的标签。我们定义 VC 维，$VC(\mathcal{H})$是最大的可粉碎样本大小。例如对于有两个维度的线性分类而言，$VC(\mathcal{H})=3$。

下式展示了对于无限假设集的定理，对于给定$\mathcal{H}$，令$d=VC(\mathcal{H})$，至少有$1-\delta$的可能性：

$$|\epsilon(h)-\hat{\epsilon}(h)| \leq O\left(\sqrt{\frac{d}{m} \log \frac{m}{d} + \frac{1}{m} \log \frac{1}{\delta}}\right)$$

同样还有下面的结论：对$\| \epsilon(h)-\hat{\epsilon}(h) \| \leq \gamma$ 要求至少$1-\delta$概率对所有$h \in \mathcal{H}$成立，需要满足$m=O_{\gamma,\delta}(d)$。即对于最小化训练误差的算法，所需的训练样本数和算法参数个数几乎成线性关系。

## 参考资料

* [康奈尔大学Machine Learning Theory课程](https://www.cs.cornell.edu/courses/cs6783/2021fa/)
* [Understanding Machine Learning: From Theory to Algorithms](https://www.amazon.com/Understanding-Machine-Learning-Theory-Algorithms/dp/1107057132)
* [cs229 Machine Learning Theory讲义](http://cs229.stanford.edu/notes_archive/cs229-notes4.pdf)
