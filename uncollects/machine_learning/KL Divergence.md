# KL Divergence

**Kullback Leibler divergence (KL divergence)** is an asymmetric divergence metric for comparing two distributions. It is defined as follows:

$$
D_{\text{KL}}(p \parallel q) \triangleq \sum_{y \in \mathcal{Y}} p(y) \log \frac{p(y)}{q(y)}
$$

KL divergence satisfied the following properties: $D_{\text{KL}}(p \parallel q) \ge 0$ with equality iff $p = q$.

$$
\begin{aligned}
D_{\mathbb{KL}}(p \parallel q) &= \sum_{y \in \mathcal{Y}} p(y) \log p(y) - \sum_{y \in \mathcal{Y}} p(y) \log q(y) \\
&= -\mathbb{H}(p) + \mathbb{H}_{\text{ce}}(p, q) \\
\mathbb{H}(p) &\triangleq -\sum_{y} p(y) \log p(y) \\
\mathbb{H}_{\text{ce}}(p, q) &\triangleq -\sum_{y} p(y) \log q(y)
\end{aligned}
$$

Here $\mathbb{H}(p)$ is the [[Entropy]], it provides the least number of bits needed to compress a dataset generated by a distribution $p$. The $\mathbb{H}_{\text{ce}}(p, q)$ is the [[Cross Entropy]], which measures the expected number of bits we need to compress a dataset coming from distribution $p$ if we design our code using distribution $q$.Thus the KL is the extra number of bits we need to use to compress the data due to using the incorrect distribution $q$.

Reference:

1. [Probabilistic Machine Learning: An Introduction](https://probml.github.io/pml-book/book1.html)
2. [Gemini](https://gemini.google.com)
