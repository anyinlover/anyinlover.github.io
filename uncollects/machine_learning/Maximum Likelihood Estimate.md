# Maximum Likelihood Estimate

When fitting probabilistic models, it is common to use the negative log probability as our loss function:

$$
\ell(y, f(\mathbf{x}; \boldsymbol{\theta})) = -\log p(y|f(\mathbf{x}; \boldsymbol{\theta}))
$$

The intuition is that a good model (with low loss) is one that assigns a high probability to the true output $y$ for each corresponding input $\mathbf{x}$. The average negative log probability of the training set is given by:

$$
\text{NLL}(\boldsymbol{\theta}) = -\frac{1}{N} \sum_{n=1}^{N} \log p(y_n|f(\mathbf{x}_n; \boldsymbol{\theta}))
$$

This is called the **negative log likelihood**. If we minimize this, we can compute the **maximum likelihood estimate** or **MLE**:

$$
\hat{\boldsymbol{\theta}}_{\text{mle}} = \underset{\boldsymbol{\theta}}{\text{argmin}} \, \text{NLL}(\boldsymbol{\theta})
$$

We can derive maximum likelihood estimate from [[KL Divergence]], which estimate the difference between two distributions.

Let $P(y | \mathbf{x})$ be the true data distribution, $Q(y | \mathbf{x}; \boldsymbol{\theta})=p(y|f(\mathbf{x}; \boldsymbol{\theta}))$ be the model distribution.

Because we can't know the real data distribution, we approximate it with the sample distribution. So we have:

$$
\begin{aligned}
D_{\mathbb{KL}}(P||Q) &= \sum P(y | \mathbf{x}) \log P(y | \mathbf{x}) - \sum P(y | \mathbf{x}) \log Q(y | \mathbf{x}; \boldsymbol{\theta}) \\
&\approx \frac{1}{N} \sum_{n=1}^N \log P(y_n | \mathbf{x}_n) - \frac{1}{N} \sum_{n=1}^N \log Q(y_n | \mathbf{x}_n; \boldsymbol{\theta}) \\
&= \text{const} + \text{NLL}(\boldsymbol{\theta})
\end{aligned}
$$

Here we can see that minimize KL Divergence is same to minimize negative log likelihood.

Reference:

1. [Probabilistic Machine Learning: An Introduction](https://probml.github.io/pml-book/book1.html)
2. [Gemini](https://gemini.google.com)

[//begin]: # "Autogenerated link references for markdown compatibility"
[KL Divergence]: <KL Divergence.md> "KL Divergence"
[//end]: # "Autogenerated link references"
