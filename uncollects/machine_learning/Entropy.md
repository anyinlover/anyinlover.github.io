# Entropy

The **entropy (Shannon entropy)** of a probability distribution can be interpreted as a measure of uncertainty.

$$
\mathbb{H}(X) \triangleq -\sum_{k=1}^{K} p(X=k) \log_2 p(X=k) = -\mathbb{E}_X[\log p(X)]
$$

Here we define **Information Content** as:

$$
I(X) = -\log p(X)
$$

It represent the uncertainty or surprise when an event happen. If an event is very likely, it's not surprising when it happens, and thus it carries less information. On the other hand, if an event is very unlikely, its occurrence is more surprising and carries more information.

So entropy is just the expected information.

Entropy is an abstract concept but can be more easily grasped in data compression. It is the least number of bits needed to compress a dataset generated by a distribution $p$.

Reference:

1. [Probabilistic Machine Learning: An Introduction](https://probml.github.io/pml-book/book1.html)
2. [ChatGPT](https://chatgpt.com)
