# Softmax Regression

In multi-class [[Classification]], there are $k$ discrete output values, $y$ follows a multinomial distribution, and can be described using Softmax regression.

$y$ can take on $k$ values, each with a probability of $\phi_1,\cdots,\phi_k$, but in reality these $k$ probabilities are not independent, with $\sum_{i=1}^{k}\phi_i = 1$. A [[Softmax Function]] is a perfect fit in this case.

We define that:

$$
P(y = i | x; \theta) = \frac{\exp(\theta_j^T x)}{\sum_{j=1}^k \exp(\theta_j^T x)}
$$

We can compute the negative log-likelihood as following:

$$
-\log P(y = i | x; \theta) = -\log (\frac{\exp(\theta_y^T x)}{\sum_{j=1}^k \exp(\theta_j^T x)})
$$


[//begin]: # "Autogenerated link references for markdown compatibility"
[Classification]: Classification.md "Classification"
[Softmax Function]: <Softmax Function.md> "Softmax Function"
[//end]: # "Autogenerated link references"
