---
category: 实践
tags:
  - 编程语言
  - 爬虫
  - Python
---

## 爬虫登录涉及的技术要点

### POST信息提取

爬虫有时候有必要登录网页，要想实现在代码中模拟登录，最主要的是模拟HTTP POST的过程。我们在浏览网站时，最常用的两个动作就是GET和POST。GET是抓取网页，POST是提交表单。这些动作Python的requests库都能实现，我们要做的就是提供素材。

在Chrome中打开www.douban.com首页，网页上右键点击Inspect打开开发者工具，切换到Network面板，勾选Preserve Log，重新加载豆瓣首页，用开发者面板左上角的选取工具选到登录框，在右侧查看源代码，可以看到豆瓣实际登录的url是https://accounts.douban.com/login，我们需要提交的账号密码和验证码，在后台分别对应四个字段：form_email，form_password，captcha-solution，captcha_id。将这四个字段值构造好，就搞定了。

### Headers和Cookies

后台模拟时需要添加的发起请求的Headers，Headers中最重要的模拟浏览器的User_Agent。Cookies则是身份认证，用于和网站通信。这两个同样可以在开发者面板中找到，当然也有Chrome小插件支持，分别是EditThisCookie和User-Agent Switcher。

## 源码分享

直接贴一下源码，对于异常没有特别处理，主要是通过两个条件来判断是否登录成功：返回是否是200和cookies是否获取到。另外要注意的是豆瓣的验证码不是每次都出现，所以需要特别处理。使用了request的session功能，cookies会自然保存在session中，后面发起请求就不需要指定cookies了。这里我找到了我和豆瓣创始人阿北的共同喜好有55个。

~~~python
#! /usr/local/bin/python3

import requests
from bs4 import BeautifulSoup
import urllib.request
import sys

headers = {
    "User-Agent":
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 "
        "Safari/537.36 "
}


def login():
    postdata = {
        "form_email": "********@163.com",
        "form_password": "********",
    }

    login_url = 'https://accounts.douban.com/login'

    session = requests.session()
    r = session.get(login_url, headers=headers)
    soap = BeautifulSoup(r.text, "html.parser")
    if soap.find(id='captcha_image'):
        captcha_url = soap.find(id='captcha_image')['src']
        captcha_id = soap.find('', {'name': 'captcha-id'})['value']
        urllib.request.urlretrieve(captcha_url, "captcha.jpg")
        postdata["captcha-solution"] = input("Read the picture number: ")
        postdata["captcha-id"] = captcha_id

    r = session.post(login_url, data=postdata, headers=headers)
    soap = BeautifulSoup(r.text, "html.parser")
    if r.status_code == 200 and r.cookies:
        print("login success")
    else:
        sys.exit("login fail, check your network, email or password")

    r = session.get("https://www.douban.com/people/1000001/", headers=headers)
    soap = BeautifulSoup(r.text, "html.parser")
    print(soap.find(id='common').h2.get_text())

    return session


login()
~~~

##  可改进点

这里很明显的一个尴尬点是验证码需要人工识别，现在听说有打码平台可以自动识别，以后有空研究研究。