---
layout: single
title: 多臂老虎机
subtitle: 强化学习系列一
date: 2021-07-25 16:22
author: Anyinlover
mathjax: true
category: 强化学习
tags:
  - 强化学习
---

## 多臂老虎机问题

多臂老虎机的问题场景很简单，面前有$k$台老虎机，不同老虎机每次拉杆的收益概率分布是固定且不同的，目标是在一定步骤后最大化总收益。

由于我们假设对老虎机的收益概率分布没有任何先验知识，我们很容易直观的用贪心算法处理这个问题-- 永远找那台当前看起来期望收益最大的机器。但这样的操作很容易陷入局部最优解中，而没有去探索更大的解空间。因此我们在游戏的过程中，除了利用之外还需要做探索这件事，去尝试一些新方案，平衡好探索和利用正是解决多臂老虎机问题的关键点。

## 平衡探索和利用的四种方法

下面是四种简单但有效的平衡探索和利用的方法。

### $\varepsilon$ 贪婪方法

定义$q_{*}(a)$为在选择动作$a$的期望收益，即价值。按定义这是一个固定值。也是我们在$t$对动作$A_t$选择$a$的收益期望。

$$ q_{*}(a) \doteq \mathbb{E}[R_t|A_t=a] $$

定义$Q_t(a)$为之前选择动作$a$的平均收益，根据大数定理，会逐渐逼近真实的$q_{*}(a)$。

$$ Q_t(a) \doteq \frac{在t之前采取a动作的收益总和}{在t之前采取a动作的次数}
    = \frac{\sum_{i=1}^{t-1}R_i \cdot \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbb{1}_{A_i=a}} $$

所谓的$\varepsilon$方法就是每次以$\varepsilon$的概率随机从所有老虎机中选择一个，而以$1 - \varepsilon$的概率选择当前$Q_t$最大的那个。前者是探索，而后者就是利用。

事实上，上面的$Q_t(a)$的计算可以用增量方法简化计算。

$$
\begin{aligned}
    Q_{t+1} &= \frac{1}{t}\sum_{i=1}^{n}R_i \\
            &= \frac{1}{t}(R_t + \sum_{i=1}^{t-1}R_i) \\
            &= \frac{1}{t}(R_t + (t-1)\frac{1}{t-1} \sum_{i=1}^{t-1}R_i) \\
            &= \frac{1}{t}(R_t + (t-1)Q_t) \\
            &= \frac{1}{t}(R_t + tQ_t-Q_t) \\
            &= Q_t + \frac{1}{t}(R_t - Q_t)
    \end{aligned}
$$

### 乐观初始值


### 上限置信区间动作选择

### 梯度算法

## 其他

### 语境赌博机

### 贝叶斯方法

## 总结

其实人生也是如此，投资亦是如此。
