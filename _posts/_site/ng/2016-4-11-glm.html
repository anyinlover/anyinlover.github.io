<p>这一章讲讲广义线性模型。我刚看到这一章的时候，觉得很神奇。知识的境界不就是归一么。能把具体的模型一般化了，这本身就是件美丽的事。这里就是把线性回归和逻辑回归都归入了广义线性模型（GLMs)。</p>

<h2 id="section">指数分布族</h2>
<p>在讲广义线性模型前，需要先了解指数分布族。凡是能表达成下面的形式的，都属于指数分布族。</p>

<script type="math/tex; mode=display">p(y;\eta)=b(y)\exp(\eta^TT(y)-a(\eta))</script>

<p>其中<script type="math/tex">\eta</script>被称为自然参数，<script type="math/tex">T(y)</script>是充分统计量，对机器学习来说一般<script type="math/tex">T(y)=y</script>，<script type="math/tex">a(\eta)</script>是对数部分函数，其作用是做一个正态化常量。</p>

<p>下面可以证明伯努利分布和高斯分布都属于指数分布族。</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
p(y;\phi)&=\phi^y(1-\phi)^{1-y}\\
&=\exp(y\log\phi+(1-y)\log(1-\phi))\\
&=\exp((\log(\frac{\phi}{1-\phi}))y+\log(1-\phi))
\end{align}
 %]]></script>

<p>因此有<script type="math/tex">\eta=\log(\frac{\phi}{1-\phi})</script>，得到<script type="math/tex">\phi=1/(1+e^{-\eta})</script>。
其他几个参数也水到渠成：</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
T(y) &= y \\
a(\eta) &= -\log(1-\phi)\\
&= \log(1+e^\eta)\\
b(y) &= 1
\end{align}
 %]]></script>

<p>对于高斯分布而言，因为<script type="math/tex">\sigma^2</script>对于最终的结果没有影响，因此取<script type="math/tex">\sigma^2=1</script>。</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
p(y;\mu)&=\frac{1}{\sqrt{2\pi}}\exp(-\frac{1}{2}(y-\mu)^2)\\
&=\frac{1}{\sqrt{2\pi}}\exp(-\frac{1}{2}y^2)\cdot\exp(\mu y-\frac{1}{2}\mu^2)
\end{align}
 %]]></script>

<p>因此有：</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
\eta &= \mu \\
T(y) &= y \\
a(\eta) &= \mu^2/2=\eta^2/2\\
b(y) &=(1/\sqrt(2\pi))\exp(-y^2/2)
\end{align}
 %]]></script>

<p>此外还有多种分布也是属于指数分布族：</p>

<ul>
  <li>多项式分布：多个离散输出建模</li>
  <li>泊松分布：对计数过程建模</li>
  <li>伽马分布和指数分布：对连续非负随机变量建模，如时间间隔</li>
  <li>贝塔分布和狄利克雷分布：对概率分布建模</li>
</ul>

<p>这么一看上次总结的七个分布完全不够用啊，逃~</p>

<h2 id="section-1">构建广义线性模型</h2>
<p>针对分类问题或回归问题，需要构造一个关于x的函数来预测y的值。满足三个假设可以构建广义线性模型：</p>

<ol>
  <li><script type="math/tex">y\mid x;\theta \sim ExponentialFamily(\eta)</script> 即y的分布要满足某些指数分布族。</li>
  <li>给定x，目标是预测<script type="math/tex">T(y)</script>的期望值，大多数情况下即y的期望值。<script type="math/tex">h(x)=E[y \mid  x]</script>。</li>
  <li>自然参数<script type="math/tex">\eta</script>和输入x成线性关系：<script type="math/tex">\eta=\theta^Tx</script></li>
</ol>

<p>这三个假说很容易理解，下面对线性回归和逻辑回归的推导也应用了三个假说，同时也证明了我们的构造的正确性。</p>

<h3 id="section-2">线性回归</h3>
<p>对于线性回归而言，其满足高斯分布，预测函数构造如下：</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
h_{\theta}(x)&=E[y \mid x;\theta]\\
&=\mu\\
&=\eta\\
&=\theta^Tx
\end{align}
 %]]></script>

<h3 id="section-3">逻辑回归</h3>
<p>对于逻辑回归而言，其满足伯努利分布，预测函数构造如下：</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
h_{\theta}(x)&=E[y \mid x;\theta]\\
&=\theta\\
&=1/(1+e^{-\eta})\\
&=1/(1+e^{-\theta^Tx})
\end{align}
 %]]></script>

<p>所以，前面逻辑回归时用逻辑斯蒂函数不是没有道理的。</p>

<h3 id="softmax-">Softmax 回归</h3>
<p>Softmax回归是逻辑回归的一般化，当输出有k个离散值时，y呈多项式分布，可以用Softmax回归刻画。</p>

<p>y可以取k个值，每一个取值的概率为<script type="math/tex">\theta_1,\cdots,\theta_k</script>，但实际上这k个概率不是相互独立的，<script type="math/tex">\theta_k=1-\sum_{i=1}^{k-1}\theta_i</script>。</p>

<p>为把多项式分布表示成指数分布族，定义<script type="math/tex">T(y)\in \mathbb{R}^{k-1}</script>，而不再是y了：</p>

<script type="math/tex; mode=display">T(1)=\begin{bmatrix}1\\0\\0\\\vdots\\0\end{bmatrix}, T(2)=\begin{bmatrix}0\\1\\0\\\vdots\\0\end{bmatrix}, T(3)=\begin{bmatrix}0\\0\\1\\\vdots\\0\end{bmatrix}, \cdots, T(k-1)=\begin{bmatrix}0\\0\\0\\\vdots\\1\end{bmatrix},T(k)=\begin{bmatrix}0\\0\\0\\\vdots\\0\end{bmatrix}</script>

<p>用一种新的表达式来表示上式，<script type="math/tex">(T(y))_i=1\{y=i\}</script>。我们可以开始证明Softmax分布也是指数分布族的一种了：</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
p(y;\phi)&=\phi_1^{1\{y=1\}}\phi_2^{1\{y=2\}}\cdots\phi_k^{1\{y=k\}}\\
&=\phi_1^{(T(y))_1}\phi_2^{(T(y))_2}\cdots\phi_k^{1-\sum_{i=1}^{k-1}(T(y))_i}\\
&=\exp((T(y))_1\log(\phi_1)+(T(y))_2\log(\phi_2)+\cdots+(1-\sum_{i=1}^{k-1}(T(y))_i)\log(\phi_k))\\
&=\exp((T(y))_1\log(\phi_1/\phi_k)+(T(y))_2\log(\phi_2/\phi_k)+\cdots+(T(y))_{k-1}\log(\phi_{k-1}/\phi_k)+\log(\phi_k))\\
\end{align}
 %]]></script>

<p>可以得出指数分布族的各参数：</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
\eta &= \begin{bmatrix}\log(\phi_1/\phi_k)\\\log(\phi_2/\phi_k)\\\vdots\\\log(\phi_{k-1}/\phi_k)\end{bmatrix}\\
a(\eta)&=-\log(\eta_k)\\
b(y)&=1
\end{align}
 %]]></script>

<p>下面需要推导从<script type="math/tex">\eta</script>到<script type="math/tex">\phi</script>的映射。
由上面可知，<script type="math/tex">\eta_i=\log(\phi_i/\phi_k)</script>，同时定义<script type="math/tex">\eta_k=\log(\phi_k/\phi_k)=0</script>。</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
e^{\eta_i}&=\frac{\phi_i}{\phi_k}\\
\phi_ke^{\eta_i}&=\phi_k\\
\phi_k\sum_{i=1}^ke^{\eta_i}&=\sum_{i=1}^k\phi_i=1\\
\phi_i&=\frac{e^{\eta_i}}{\sum_{j=1}^k e^{\eta_j}}
\end{align}
 %]]></script>

<p>上面的表达式可以求解<script type="math/tex">\phi_1,\cdots,\phi_{k-1}</script>，<script type="math/tex">\phi_k=1/\sum_{i=1}^ke^{\eta_i}</script>，这个从<script type="math/tex">\eta</script>到<script type="math/tex">\phi</script>的映射函数称为softmax函数。</p>

<p><script type="math/tex">\eta</script>和x还是线性关系，<script type="math/tex">\eta_i=\theta_i^Tx,(i=1,\cdots,k-1)</script>，其中<script type="math/tex">\theta_1,\cdots,\theta_{k-1}\in \mathbb{R}^{n+1}</script>是我们模型的参数，定义<script type="math/tex">\theta_k=0</script>，则<script type="math/tex">\eta_k=\theta_k^Tx=0</script>，因此分布概率可表达成下式：</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
p(y=i \mid x;\theta) &= \phi_i\\
&=\frac{e^{\eta_i}}{\sum_{j=1}^ke^{\eta_j}}\\
&=\frac{e^{\theta_i^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}}
\end{align}
 %]]></script>

<p>我们的预测函数则可以表达为下式：</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
h_{\theta}(x) &= E[T(y) \mid x;\theta]\\
&= \begin{bmatrix}\phi_1\\\phi_2\\\vdots\\\phi_{k-1}\end{bmatrix}\\
&=\begin{bmatrix}\frac{e^{\theta_1^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}}\\
\frac{e^{\theta_2^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}}\\
\vdots\\
\frac{e^{\theta_{k-1}^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}}\end{bmatrix}
\end{align}
 %]]></script>

<p>最后讨论一下参数拟合。还是采用和逻辑回归同样的方法，求最大似然函数的最大值，可以使用梯度下降法或牛顿法：</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
\ell(\theta)&=\sum_{i=1}^m\log p(y^{(i)} \mid x^{(i)};\theta)\\
&=\sum_{i=1}^m\log \prod_{i=1}^k(\frac{e^{\theta_l^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}})^{1\{y^{(i)}=l\}}
\end{align}
 %]]></script>
