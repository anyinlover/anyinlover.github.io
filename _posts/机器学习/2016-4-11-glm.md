---
layout: single
title: "广义线性模型"
subtitle: "斯坦福大学机器学习第三讲"
date: 2016-4-11
author: "Anyinlover"
mathjax: true
category: 机器学习
tags:
  - Ng机器学习系列
  - 机器学习算法
---

这一章讲讲广义线性模型。我刚看到这一章的时候，觉得很神奇。知识的境界不就是归一么。能把具体的模型一般化了，这本身就是件美丽的事。这里就是把线性回归和逻辑回归都归入了广义线性模型（GLMs)。

## 指数分布族

在讲广义线性模型前，需要先了解指数分布族。凡是能表达成下面的形式的，都属于指数分布族。

$$p(y;\eta)=b(y)\exp(\eta^TT(y)-a(\eta))$$

其中$$\eta$$被称为自然参数，$$T(y)$$是充分统计量，对机器学习来说一般$$T(y)=y$$，$$a(\eta)$$是对数部分函数，其作用是做一个正态化常量。

下面可以证明伯努利分布和高斯分布都属于指数分布族。

$$
\begin{align}
p(y;\phi)&=\phi^y(1-\phi)^{1-y}\\
&=\exp(y\log\phi+(1-y)\log(1-\phi))\\
&=\exp((\log(\frac{\phi}{1-\phi}))y+\log(1-\phi))
\end{align}
$$

因此有$$\eta=\log(\frac{\phi}{1-\phi})$$，得到$$\phi=1/(1+e^{-\eta})$$。
其他几个参数也水到渠成：

$$
\begin{align}
T(y) &= y \\
a(\eta) &= -\log(1-\phi)\\
&= \log(1+e^\eta)\\
b(y) &= 1
\end{align}
$$

对于高斯分布而言，因为$$\sigma^2$$对于最终的结果没有影响，因此取$$\sigma^2=1$$。

$$
\begin{align}
p(y;\mu)&=\frac{1}{\sqrt{2\pi}}\exp(-\frac{1}{2}(y-\mu)^2)\\
&=\frac{1}{\sqrt{2\pi}}\exp(-\frac{1}{2}y^2)\cdot\exp(\mu y-\frac{1}{2}\mu^2)
\end{align}
$$

因此有：

$$
\begin{align}
\eta &= \mu \\
T(y) &= y \\
a(\eta) &= \mu^2/2=\eta^2/2\\
b(y) &=(1/\sqrt(2\pi))\exp(-y^2/2)
\end{align}
$$

此外还有多种分布也是属于指数分布族：

- 多项式分布：多个离散输出建模
- 泊松分布：对计数过程建模
- 伽马分布和指数分布：对连续非负随机变量建模，如时间间隔
- 贝塔分布和狄利克雷分布：对概率分布建模

这么一看上次总结的七个分布完全不够用啊，逃~

## 构建广义线性模型

针对分类问题或回归问题，需要构造一个关于 x 的函数来预测 y 的值。满足三个假设可以构建广义线性模型：

1. $$y\mid x;\theta \sim ExponentialFamily(\eta)$$ 即 y 的分布要满足某些指数分布族。
2. 给定 x，目标是预测$$T(y)$$的期望值，大多数情况下即 y 的期望值。$$h(x)=E[y \mid  x]$$。
3. 自然参数$$\eta$$和输入 x 成线性关系：$$\eta=\theta^Tx$$

这三个假说很容易理解，下面对线性回归和逻辑回归的推导也应用了三个假说，同时也证明了我们的构造的正确性。

### 线性回归

对于线性回归而言，其满足高斯分布，预测函数构造如下：

$$
\begin{align}
h_{\theta}(x)&=E[y \mid x;\theta]\\
&=\mu\\
&=\eta\\
&=\theta^Tx
\end{align}
$$

### 逻辑回归

对于逻辑回归而言，其满足伯努利分布，预测函数构造如下：

$$
\begin{align}
h_{\theta}(x)&=E[y \mid x;\theta]\\
&=\theta\\
&=1/(1+e^{-\eta})\\
&=1/(1+e^{-\theta^Tx})
\end{align}
$$

所以，前面逻辑回归时用逻辑斯蒂函数不是没有道理的。

### Softmax 回归

Softmax 回归是逻辑回归的一般化，当输出有 k 个离散值时，y 呈多项式分布，可以用 Softmax 回归刻画。

y 可以取 k 个值，每一个取值的概率为$$\theta_1,\cdots,\theta_k$$，但实际上这 k 个概率不是相互独立的，$$\theta_k=1-\sum_{i=1}^{k-1}\theta_i$$。

为把多项式分布表示成指数分布族，定义$$T(y)\in \mathbb{R}^{k-1}$$，而不再是 y 了：

$$T(1)=\begin{bmatrix}1\\0\\0\\\vdots\\0\end{bmatrix}, T(2)=\begin{bmatrix}0\\1\\0\\\vdots\\0\end{bmatrix}, T(3)=\begin{bmatrix}0\\0\\1\\\vdots\\0\end{bmatrix}, \cdots, T(k-1)=\begin{bmatrix}0\\0\\0\\\vdots\\1\end{bmatrix},T(k)=\begin{bmatrix}0\\0\\0\\\vdots\\0\end{bmatrix}$$

用一种新的表达式来表示上式，$$(T(y))_i=1\{y=i\}$$。我们可以开始证明 Softmax 分布也是指数分布族的一种了：

$$
\begin{align}
p(y;\phi)&=\phi_1^{1\{y=1\}}\phi_2^{1\{y=2\}}\cdots\phi_k^{1\{y=k\}}\\
&=\phi_1^{(T(y))_1}\phi_2^{(T(y))_2}\cdots\phi_k^{1-\sum_{i=1}^{k-1}(T(y))_i}\\
&=\exp((T(y))_1\log(\phi_1)+(T(y))_2\log(\phi_2)+\cdots+(1-\sum_{i=1}^{k-1}(T(y))_i)\log(\phi_k))\\
&=\exp((T(y))_1\log(\phi_1/\phi_k)+(T(y))_2\log(\phi_2/\phi_k)+\cdots+(T(y))_{k-1}\log(\phi_{k-1}/\phi_k)+\log(\phi_k))\\
\end{align}
$$

可以得出指数分布族的各参数：

$$
\begin{align}
\eta &= \begin{bmatrix}\log(\phi_1/\phi_k)\\\log(\phi_2/\phi_k)\\\vdots\\\log(\phi_{k-1}/\phi_k)\end{bmatrix}\\
a(\eta)&=-\log(\eta_k)\\
b(y)&=1
\end{align}
$$

下面需要推导从$$\eta$$到$$\phi$$的映射。
由上面可知，$$\eta_i=\log(\phi_i/\phi_k)$$，同时定义$$\eta_k=\log(\phi_k/\phi_k)=0$$。

$$
\begin{align}
e^{\eta_i}&=\frac{\phi_i}{\phi_k}\\
\phi_ke^{\eta_i}&=\phi_k\\
\phi_k\sum_{i=1}^ke^{\eta_i}&=\sum_{i=1}^k\phi_i=1\\
\phi_i&=\frac{e^{\eta_i}}{\sum_{j=1}^k e^{\eta_j}}
\end{align}
$$

上面的表达式可以求解$$\phi_1,\cdots,\phi_{k-1}$$，$$\phi_k=1/\sum_{i=1}^ke^{\eta_i}$$，这个从$$\eta$$到$$\phi$$的映射函数称为 softmax 函数。

$$\eta$$和 x 还是线性关系，$$\eta_i=\theta_i^Tx,(i=1,\cdots,k-1)$$，其中$$\theta_1,\cdots,\theta_{k-1}\in \mathbb{R}^{n+1}$$是我们模型的参数，定义$$\theta_k=0$$，则$$\eta_k=\theta_k^Tx=0$$，因此分布概率可表达成下式：

$$
\begin{align}
p(y=i \mid x;\theta) &= \phi_i\\
&=\frac{e^{\eta_i}}{\sum_{j=1}^ke^{\eta_j}}\\
&=\frac{e^{\theta_i^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}}
\end{align}
$$

我们的预测函数则可以表达为下式：

$$
\begin{align}
h_{\theta}(x) &= E[T(y) \mid x;\theta]\\
&= \begin{bmatrix}\phi_1\\\phi_2\\\vdots\\\phi_{k-1}\end{bmatrix}\\
&=\begin{bmatrix}\frac{e^{\theta_1^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}}\\
\frac{e^{\theta_2^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}}\\
\vdots\\
\frac{e^{\theta_{k-1}^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}}\end{bmatrix}
\end{align}
$$

最后讨论一下参数拟合。还是采用和逻辑回归同样的方法，求最大似然函数的最大值，可以使用梯度下降法或牛顿法：

$$
\begin{align}
\ell(\theta)&=\sum_{i=1}^m\log p(y^{(i)} \mid x^{(i)};\theta)\\
&=\sum_{i=1}^m\log \prod_{i=1}^k(\frac{e^{\theta_l^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}})^{1\{y^{(i)}=l\}}
\end{align}
$$
