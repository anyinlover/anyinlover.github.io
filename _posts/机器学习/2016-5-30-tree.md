---
layout: single
title: "树"
subtitle: "ESL第9.2节"
date: 2016-5-30
author: "Anyinlover"
mathjax: true
category: 机器学习
tags:
  - ESL
  - 机器学习算法
---

## 背景

基于树的方法将特征空间分割成一系列长方形，然后用一个简单的模型（类似于常量）来拟合。树的概念简单，但往往很有效。我们首先来看一种最常见的基于树的回归分类算法 CART。

CART 是一类迭代二叉树，其关键优势在于其可解释性。每次都只分割一个特征。

## 回归树

现在我们来讨论如何构造一棵回归树。对于 N 次观察，我们的数据由 p 个输入和一个输出构成。即$$(x_i,y_i), \text{ for } i=1,2,\cdots,N, \text{ with } x_i=(x_{i1},x_{x2},\cdots,x_{ip})$$。算法需要自动确定分割的变量和分割点，以及树的模型。首先假设我们分割成 M 个区域$$R_1,R_2,\cdots,R_M$$，将每个区域映射成一个常量$$c_m$$：

$$f(x) = \sum_{m=1}^M c_m I(x \in R_m)$$

如果我们采用最小二乘法$$\sum (y_i - f(x_i))^2 $$，很容易得到最好的$$\hat{c}_m$$就是区域$$R_m$$中$$y_i$$的平均值：

$$\hat{c}_m = ave(y_i \mid x_i \in R_m)$$

我们使用贪婪算法来找出最优的分割变量 j 和分割点 s，定义如下一对分割面：

$$R_1(j,s)=\{X \mid X_j \leq s\} \text{ and } R_2(j,s)=\{X \mid X_j > s\}$$

通过解决下式来找到分割变量 j 和分割点 s：

$$
\min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i - c_2)^2]$$

通过不断迭代，可以完成树的构造。一般在达到最小节点数目时停止构造。树的大小是控制模型复杂度的一个重要调优参数。最常使用的策略是构造一个大的树$$T_0$$，然后使用代价复杂度对树进行剪枝。

我们定义$$T \subset T_0$$为任意一个可以通过对$$T_0$$剪枝得到的子树，令叶节点索引为m，节点m代表着分区$$R_m$$，$$\mid T \mid$$代表叶节点的数目。令：
$$

\begin{align}
N*m &= \#\{ x_i \in R_m\} \\
\hat{c}\_m &= \frac{1}{N_m} \sum*{x*i \in R_m} y_i \\
Q_m(T) &= \frac{1}{N_m} \sum*{x_i \in R_m} (y_i - \hat{c}\_m)^2
\end{align}

$$
定义代价复杂度公式为：
$$

这意味着树的大小和拟合度之间的平衡。$$\alpha$$越小，树越大，否则相反。当$$\alpha=0$$时，结果就是一个满树$$T_0$$。

对于任一个$$\alpha$$都有一个特定的最小子树$$T_\alpha$$使得$$C_\alpha(T)$$最小。我们使用最弱连接剪枝算法：每一次拿走一个中间的节点使得$$\sum_m N_m Q_m(T)$$增加最小，直至只留下根节点。可以证明我们要的最小子树必然在这一系列树中。

$$\alpha$$的选择可以通过交叉验证来择取。

## 分类树

对于需要输出分类结果的分类树而言，和决策树相比唯一需要改变的就是分割节点和修剪树的算法。对节点 m 而言，$$N_m$$次观测区域$$R_m$$，令：

$$\hat{p}_{mk} = \frac{1}{N_m} \sum_{x_i \in R_m} I (y_i=k)$$

作为 k 类样本在节点 m 中的比例，我们将节点 m 分类为$$k(m)= arg max_k \hat{p}_{mk}$$。对于不纯度$$Q_m(T)$$主要有三种衡量方法：

误分类率：

$$\frac{1}{N_m} \sum_{x_i \in R_m} I (y_i \neq k(m)) = 1 - \hat{p}_{mk(m)}$$

gini 系数：

$$\sum_{k \neq k'} \hat{p}_{mk} \hat{p}_{mk'} = \sum_{k=1}^K \hat{p}_{mk} (1 - \hat{p}_{mk}) $$

互熵：

$$ - \sum_{k=1}^K \hat{p}_{mk} \log \hat{p}_{mk} $$

其中 gini 系数和互熵是可导的，因此更适合做数值优化。此外和回归树类似，在分割后的两个节点里都需要计算不纯度。

此外 gini 系数和互熵对节点概率的变化更加敏感。因此常常使用 gini 系数和互熵来生成树。三种方法都可以应用在代价复杂度剪枝上。但最常用的是误分类率。

gini 系数有另外两种有趣的解释。如果不以主要类的角度去看，对于每一个类 k 而言都有可能性$$\hat{p}_{mk}$$，因此总的误分类率是$$\sum_{k \neq k'} \hat{p}_{mk} \hat{p}_{mk'}$$。类似的，将每一个类都看做一个伯努利分布，则其方差是$$\hat{p}_{mk} (1 - \hat{p}_{mk})$$，对所有类累加也得到 gini 系数。

## 其他讨论

### 分类预测

如果分类器有 q 个可能值，那么分成两组时就会有$$2^{q-1}-1$$种可能的分法，当 q 很大时，结果会非常巨大。当结果只是 0-1 时，那么分类就会很简单。这对计算 gini 系数或互熵而言同样如此。

分组的算法更青睐有更多层数的分类器，分组更多，就更有可能找到一个吻合数据的结构。但同时也可能发生过拟合。

### 损失矩阵

在分类问题中，不同类的误分类严重性不一致。我们定义一个$$K \times K$$的损失矩阵$$L$$，其中$$L_{kk'}$$代表将$$k'$$误分类成$$k$$。在模型中，可以将 gini 系数修改成$$\sum_{k \neq k'} L_{kk'} \hat{p}_{mk} \hat{p}_{mk'}$$。这种方法只使用于多分类。对于二分类而言，选择$$L_{kk'}$$。在叶节点，我们定义类$$k(m)=arg min_k \sum_\ell L_{\ell k} \hat{p}_{m \ell}$$。

### 预测值缺失

仅仅将预测值缺失的样本删除可能导致样本数不够。有两种更好的方法解决。第一种应用于分类预测中，简单的将缺失值分为一个新的类。第二种是首先使用预测值不缺失的样本构造一个首要分类器，然后再构建一系列的替代分类器，以和首要分类器偏差小为序。这种预测值的替代试图来减少预测值缺失的影响。

### 为什么使用二分法

我们也可以每一次分成多个组。但这样有可能造成分组过快，在下一层没有足够多的数据来分了。由于一次分多组相当于多次二分，因此更应该选择二分法。

### 其他构建树的方法

我们前面的讨论是树的 CART 实现，其他常用的方法还有 ID3，C4.5，C5.0。演化到 C5.0 已经和 CART 类似，一个 C5.0 重要的特征是有一组规则组，随着树的增长，分类规则可以简化，直到定义完叶节点。

### 线性组合分割

除了限制分割以$$X_j \leq s$$形式，还可以用线性组合的形式$$\sum a_j X_j \leq s$$。这里的权重$$a_j$$和分割点 s 都通过最小化不纯度（比如 gini 系数）来得到。这种方法提升了其预测能力，但损失了解释性。一种更好的方法称为 HME。

### 树的不稳定性

树的主要问题在于其高方差。一个小的改变可能造成一个非常不同的分割。主要原因在于其继承的本质。上层分割的误差会被传递到其后的下层。Bagging 算法可以通过构造多棵树来减少方差。

### 缺失平滑性

对于分类树而言，这种平滑性缺失影响不大，但对于回归树而言，却是一个很大的限制。MARS 是一种针对此改进的方法。

### 可加结构添加困难

对于添加了噪音的树结构而言，很难去表达这种噪音。MARS 放弃了这种树结构来捕获可加结构。
