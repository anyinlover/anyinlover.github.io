---
title: 爬虫使用代理
category: 实践
tags: 编程语言 爬虫 Python
---

## 爬虫代理

很多网站都会反爬虫，有代理IP池的话可以轻松突破。但一直没找到免费可用的代理IP，下面这代码我在公司内用公司代理轻松爬取。

## 源码

~~~python
#!/usr/local/bin/python3

import requests
from bs4 import BeautifulSoup
import random
import logging


headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) "
                         "Chrome/63.0.3239.132 Safari/537.36"}

logging.basicConfig(filename="lxf_git.log", format='%(asctime)s %(levelname)s: %(message)s', level=logging.DEBUG)

aimdir = 'lxf_git/'


def randomproxy():
    pros = ["183.152.50.139:8118", "122.114.31.177:808", "61.135.217.7:80", "59.173.211.127:8118",
            "112.255.5.52:8118", "112.93.123.115:8118", "218.73.138.64:44258", "1120.15.156.56:8118"
            ]
    pro = random.choice(pros)
    proxy = {"http": pro, "https": pro}
    return proxy


def downfile(url, filename):
    try:
        proxies = randomproxy()
        print(proxies)
        r = requests.get(url, headers=headers, proxies=proxies, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")
        content = soup.find('', {'class': 'x-wiki-content x-main-content'})
        for img in content.findAll('img'):
            r = requests.get(img['data-src'], headers=headers, proxies=proxies, timeout=10)
            imgname = img['alt']+'.jpg'
            with open(aimdir+'img/'+imgname, 'wb') as file:
                file.write(r.content)
            logging.info(imgname + " download successfully!")
            img['src'] = 'img/' + imgname
        with open(aimdir+filename, 'w') as file:
            file.write("<!DOCTYPE HTML>\n")
            file.write("<HTML>\n")
            file.write("<BODY>\n")
            file.write(content.prettify())
            file.write("</BODY>\n")
            file.write("</HTML>\n")
        logging.info(filename + " parse successfully!")
    except:
        downfile(url, filename)


def writetoc(toc):
    with open(aimdir+"index.html", 'w') as file:
        file.write("<!DOCTYPE HTML>\n")
        file.write("<HTML>\n")
        file.write("<BODY>\n")
        file.write(toc.prettify())
        file.write("</BODY>\n")
        file.write("</HTML>\n")
    logging.info("index.html write successfully!")


def mainspider():
    indexUrl = "https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000"
    r = requests.get(indexUrl, headers=headers, timeout=10)
    soup = BeautifulSoup(r.text, "html.parser")
    toc = soup.find(id="x-wiki-index").div
    for div in toc.findAll('div'):
        div['style'] = "position:relative;margin-left:15px"
    for link in toc.findAll('a'):
        filename = link.get_text()+".html"
        downfile("https://www.liaoxuefeng.com"+link.attrs['href'], filename)
        link.attrs['href'] = filename
    writetoc(toc)


mainspider()
~~~

