<!---
id: undefined
title: 提升方法
tags:
  - 机器学习
theme: dark
 --->
# 提升方法

[toc]
## 提升方法的理论基础

提升方法的理论基础是在概率近似正确的学习框架下，强可学习和弱可学习是等价的，一个类只要是弱可学习的就可以转化为强可学习的。一个类如果存在一个多项式学习算法能够学习它，并且正确率很高，则称为强可学习的，正确率仅比随机好点，则称为弱可学习的。

所谓的提升方法就是把弱学习算法转变为强学习方法。

## AdaBoost算法

经典的AdaBoost算法主要在两个方面来实现提升。

- 通过提升前一轮弱分类器错误分类样本权值降低正确分类样本权值使得这部分错误分类数据在后一轮弱分类器中得到更大关注。
- 弱分类器的组合加权表决，分类误差率大的权值低。

AdaBoost算法的步骤如下：

1. 初始化训练数据权值分布：

    $$ D_1 = (\omega_{11},\dotsc,\omega_{1i},\dotsc,\omega_{1N}), \omega_{1i} = \frac{1}{N}, i=1,2,\dotsc,N$$

2. 对$m = 1,2,\dotsc,M$
   
    a. 通过带权值分布$D_m$的训练数据集训练，得到基本分类器：

    $$ G_m(x): \chi \to \{-1, +1\} $$

    b. 计算$G_m(x)$ 在训练数据集上的分类误差率

    $$ e_m = \sum_{i=1}^N P(G_m(x_i) \neq y_i) = \sum_{i=1}^N \omega_{mi}I(G_m(x_i) \neq y_i) $$

    c. 计算$G_m(x)$的系数

    $$ \alpha_m = \frac{1}{2}log \frac{1-e_m}{e_m} $$

    d. 更新权值分布

    $$ \omega_{m+1,i} = \frac{\omega_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)) $$


3. 构建基本分类器的线性组合得到分类器：

$$ G(x) = sing(f(x)) = sign(\sum_{m=1}^M \alpha_m G_m(x)) $$

## AdaBoost训练误差分析

AdaBoost最吸引人的就是在迭代过程中可以不断减少训练误差。有如下定理。

$$ \frac{1}{N} \sum_{i=1}^N I(G(x_i) \neq y_i) \leq \frac{1}{N} \sum_{i} \exp(-y_i f(x_i)) = \prod_m Z_m $$

其中不等式前半部分直接根据定义可以得到，后半部分根据$Z_m$的定义式也可以得到：

$$
\begin{aligned}
\frac{1}{N}\sum_i \exp(-y_if(x_i)) &= \frac{1}{N}\sum_i\exp(-\sum_{m=1}^M \alpha_m y_i G_m(x_i)) \\
&= \sum_{i}\omega_{1i} \prod_{m=1}^M exp(-\alpha_m y_i G_m(x_i)) \\
&= Z_1 \sum_{i}\omega_{2i} \prod_{m=2}^M exp(-\alpha_m y_i G_m(x_i)) \\
&= Z_1Z_2 \sum_{i}\omega_{3i} \prod_{m=3}^M exp(-\alpha_m y_i G_m(x_i)) \\
&= \dotsb \\
&= \prod_{m=1}^M Z_m
\end{aligned}
$$

在二分类情况下，进一步可以证明AdaBoost的误差是指数收敛的。

$$
\begin{aligned}
Z_m &= \sum_{i=1}^N \omega_{mi} \exp(-\alpha_my_iG_m(x_i)) \\
&= \sum_{y_i=G_m(x_i)} \omega_{mi} e^{-\alpha_m} + \sum_{y_i \neq G_m(x_i)} \omega_{mi} e^{\alpha_m} \\
&= (1-e_m)e^{-\alpha_m} + e_me^{\alpha_m} \\
&= 2\sqrt{e_m(1-e_m)} \\
&= \sqrt(1-4\gamma_m^2)
\end{aligned}
$$

这里$\gamma_m = \frac{1}{2} - \alpha_m$，根据泰勒不等式可知：

$$ \prod_{m=1}^M \sqrt{1-4\gamma_m^2} \leq \exp(-2\sum_{m=1}^M \gamma_m^2) $$

## AdaBoost的前向分布算法解释

AdaBoost可以被纳入更大的算法框架前向分布算法中，AdaBoost可被视作其特例。

$$ f(x) = \sum_{m=1}^M \beta_m b(x; \gamma_m)$$

对于上面加法模型而言，损失函数极小化的问题可以通过每次只学一个基函数逐步逼近。

对于训练数据集$T = \{(x_1, y_1), (x_2, y_2), \dotsc, (x_N, y_N)\}$，损失函数$L(y, f(x))$，基函数集$\{b(x;r\}$：

1. 初始化$f_0(x) = 0$
2. 对$m = 1,2,\dotsc,M$
   
   极小化损失函数得到参数$\beta_m, \gamma_m$。

   $$ (\beta_m, \gamma_m) = \arg \min_{\beta,\gamma} \sum_{i=1}^N L(y_i, f_{m-1}(x_i) + \beta b(x_i;\gamma)) $$

   更新

   $$ f_m(x) = f_{m-1}(x) + \beta_m b(x; \gamma_m) $$

3. 得到加法模型

    $$ f(x) = f_M(x) = \sum_{m=1}^M \beta_m b(x;\gamma_m)

可以证明AdaBoost相当于由基本分类器组成的加法模型，损失函数是指数函数。

## 提升树

对于前述前向分布算法，当基函数是决策树时就是提升树算法。对于分类问题其求解方式与AdaBoost类似，对于用平方误差损失函数的回归问题，有很好的求解性质。

回归问题提升树使用以下前向分布算法：

$$
\begin{aligned}
f_0(x) &= 0 \\
f_m(x) &= f_{m-1}(x) + T(x;\varTheta_m), m = 1,2,\dotsc, M \\
f_M(x) &= \sum_{m=1}^M T(x; \varTheta_m)
\end{aligned}
$$

在前向分布算法的第m步，即求解

$$ \hat\varTheta_m = \arg\min_{\varTheta_m} \sum_{i=1}^N L(y_i, f_{m-1}(x_i) + T(x_i; \varTheta_m)) $$

对于平方误差损失函数而言：

$$L(y,f(x)) = (y-f(x))^2$$

其损失变为：

$$
\begin{aligned} 
L(y_i, f_{m-1}(x) + T(x; \varTheta_m)) &= [y - f_{m-1}(x) - T(x; \varTheta_m)]^2 \\
&= [r - T(x; \varTheta_m)]^2
\end{aligned}
$$

这里：

$$ r = y - f_{m-1}(x) $$

所以对于平方误差损失函数而言，下一颗树直接拟合当前模型的残差即可，计算非常方便。

### GBDT

GBDT赫赫有名，其实就是上面思想对于一般损失函数的扩展，对于一般损失函数而言，无法像平方误差损失函数那样直接用残差拟合，就用损失函数的负梯度作为近似：

$$ r = -[\frac{\partial L(y, f(x))}{\partial f(x)}]_{f(x)=f_{m-1}(x)} $$

整体算法过程如下：

1. 初始化：

    $$ f_0(x) = \arg \min_c \sum_{i=1}^N L(y_i, c) $$

2. 对 $m = 1,2,\dotsc,M$
   1. 对 $i=1,2,\dotsc,N$，计算：

        $$ r_{mi} = -[\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)} $$

    2. 对$r_{mi}$ 拟合一个回归树，得到叶节点区域$R_{mi}, j=1,2,\dotsc,J$。
    3. 对 $j=1,2,\dotsc,J$，计算区域内损失函数极小化的$c_{mj}$：

        $$ c_{mj} = \arg \min_c \sum_{i=1}^N L(y_i, c) $$
   
    4. 更新$f_m(x) = f_{m-1}(x) + \sum_{j=1}^Jc_{mj}I(x \in R_{mj})$

3. 最终得到回归树：
   
    $$ \hat{f}(x)= f_M(x) = \sum_{m=1}^M \sum_{j=1}^J c_{mj} I(x \in R_{mj}) $$
